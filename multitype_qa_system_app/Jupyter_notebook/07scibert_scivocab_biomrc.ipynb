{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab539ac-590f-43f6-a57c-aace35a625dd",
   "metadata": {},
   "source": [
    "# PREDICT USING PRETRAINED MODEL #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906bddd-d599-4672-be74-f02ee1db9082",
   "metadata": {},
   "source": [
    "#### GPU AND LIBRARIES ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ad487-2d51-419d-bff7-402e433f287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f6a5c-faa0-4c17-980e-da9f5de85117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "__author__ = 'Petros'\n",
    "\n",
    "my_seed = 1989\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "random.seed(my_seed)\n",
    "torch.manual_seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "embedding_dim = 30\n",
    "hidden_dim = 100\n",
    "gpu_device = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if (use_cuda):\n",
    "    torch.cuda.manual_seed(my_seed)\n",
    "    print(\"Using GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe86dc-821f-47e2-b1a7-707628fa67a4",
   "metadata": {},
   "source": [
    "#### DECLARATION OF FUNCTIONS, CLASS AND METHODS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf2eaa-2735-4be8-b530-8aaa5bc5f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN FUNCTIONS\n",
    "def print_params(show_model=True):\n",
    "    if show_model:\n",
    "        print(40 * '=')\n",
    "        print(model)\n",
    "    print(40 * '=')\n",
    "    total_params = 0\n",
    "    print('Trainable Parameters\\n')\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            v = 1\n",
    "            for s in parameter.size():\n",
    "                v *= s\n",
    "            total_params += v\n",
    "    print(40 * '=')\n",
    "    print(total_params)\n",
    "    print(40 * '=')\n",
    "\n",
    "\n",
    "class SciBertReaderSum(nn.Module):\n",
    "    def __init__(self, frozen_top):\n",
    "        super(SciBertReaderSum, self).__init__()\n",
    "\n",
    "        self.tok = BertTokenizer.from_pretrained('./biomrc/scibert_scivocab_uncased/')\n",
    "        self.bert = BertModel.from_pretrained('./biomrc/scibert_scivocab_uncased/')\n",
    "        self.linear = nn.Linear(2 * 768, 100, bias=True)\n",
    "        self.linear2 = nn.Linear(100, 1, bias=True)\n",
    "\n",
    "        self.frozen_top = frozen_top\n",
    "\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        if use_cuda:\n",
    "            self.bert = self.bert.cuda(gpu_device)\n",
    "            self.linear = self.linear.cuda(gpu_device)\n",
    "            self.linear2 = self.linear2.cuda(gpu_device)\n",
    "\n",
    "    def freeze_top(self, optim):\n",
    "        for p in self.bert.encoder.layer[-1].parameters():\n",
    "            p.requires_grad = False\n",
    "        for g in optim.param_groups:\n",
    "            g['lr'] = 0.001\n",
    "        self.frozen_top = True\n",
    "\n",
    "    def unfreeze_top(self, optim):\n",
    "        for p in self.bert.encoder.layer[-1].parameters():\n",
    "            p.requires_grad = True\n",
    "        for g in optim.param_groups:\n",
    "            g['lr'] = 0.0001\n",
    "        self.frozen_top = False\n",
    "\n",
    "    def fix_input(self, abstract, title):\n",
    "        ab_sents = sent_tokenize(abstract, 'english')\n",
    "        ab_tok = [self.tok.tokenize(ab) for ab in ab_sents]\n",
    "        entity_indices = list()\n",
    "        entity_texts = list()\n",
    "        for i in range(len(ab_tok)):\n",
    "            ab_tok[i].insert(0, '[CLS]')\n",
    "            entity_indices.append(list())\n",
    "            entity_texts.append(list())\n",
    "            for j in range(len(ab_tok[i])):\n",
    "                if ab_tok[i][j] == '@':\n",
    "                    if ab_tok[i][j+1] == 'entity':\n",
    "                        entity_indices[-1].append(j)\n",
    "            for j in entity_indices[-1]:\n",
    "                n = j+1\n",
    "                while ab_tok[i][n].startswith('##') or ab_tok[i][n] == 'entity':\n",
    "                    n += 1\n",
    "                    if n >= len(ab_tok[i]):\n",
    "                        break\n",
    "                entity_texts[-1].append(''.join(ab_tok[i][j:n]).replace('##', ''))\n",
    "        ti_tok = self.tok.tokenize(title.replace('.XXXX', ' [MASK]').replace('XXXX', '[MASK]'))\n",
    "        ti_tok.insert(0, '[SEP]')\n",
    "        combined = list()\n",
    "        mask_indices = list()\n",
    "        for i in range(len(ab_tok)):\n",
    "            combined.append(list())\n",
    "            combined[i].extend(ab_tok[i])\n",
    "            combined[i].extend(ti_tok)\n",
    "            mask_indices.append(combined[i].index('[MASK]'))\n",
    "        combined_inp = [torch.LongTensor(self.tok.convert_tokens_to_ids(c)).unsqueeze(dim=0) for c in combined]\n",
    "\n",
    "        if use_cuda:\n",
    "            combined_inp = [e.cuda(gpu_device) for e in combined_inp]\n",
    "\n",
    "        return combined_inp, mask_indices, entity_indices, entity_texts\n",
    "\n",
    "    def forward(self, abstract, title, entity_list, answer, ignore_big=True):\n",
    "        combined_input, mask_indices, entity_indices, entity_texts = self.fix_input(abstract, title)\n",
    "        max_len = max([e.shape[-1] for e in combined_input])\n",
    "        if ignore_big and max_len > 512:\n",
    "            return None, None\n",
    "        # Pad combined input\n",
    "        combined_input = torch.stack([F.pad(e, [0, max_len - e.shape[-1], 0, 0], 'constant', 0) for e in combined_input], dim=0).squeeze(dim=1)\n",
    "        if use_cuda:\n",
    "            combined_input = combined_input.cuda(gpu_device)\n",
    "        out = list()\n",
    "        bert_out = self.bert(combined_input)[0][-1]\n",
    "        for i in range(combined_input.shape[0]):\n",
    "            if len(entity_indices[i]) == 0:\n",
    "                continue\n",
    "            if len(entity_indices[i]) != 1:\n",
    "                bert_out_entities = bert_out[i, entity_indices[i], :].squeeze(dim=0)\n",
    "            else:\n",
    "                bert_out_entities = bert_out[i, entity_indices[i], :]\n",
    "            bert_out_mask = bert_out[i, mask_indices[i], :]\n",
    "            bert_out_mask = bert_out_mask.expand_as(bert_out_entities)\n",
    "            bert_out_concat = torch.cat([bert_out_entities, bert_out_mask], dim=-1)\n",
    "            out.append(self.linear2(F.relu(self.linear(bert_out_concat))))\n",
    "        entity_texts = [e for e in entity_texts if len(e) != 0]\n",
    "        # Predict also\n",
    "        preds = dict()\n",
    "        for et, ot in zip(entity_texts, out):\n",
    "            ot = ot.detach().cpu().numpy().tolist()\n",
    "            for e, o in zip(et, ot):\n",
    "                if e not in preds:\n",
    "                    preds[e] = 0\n",
    "                preds[e] += o[0]\n",
    "        entity_outs = dict()\n",
    "        for ent in entity_list:\n",
    "            entity_outs[ent] = list()\n",
    "            for r_i, r in enumerate(entity_texts):\n",
    "                for c_i, c in enumerate(r):\n",
    "                    if c == ent:\n",
    "                        entity_outs[ent].append(out[r_i][c_i])\n",
    "        for ent in list(entity_outs):\n",
    "            if len(entity_outs[ent]) == 0:\n",
    "                del entity_outs[ent]\n",
    "            else:\n",
    "                entity_outs[ent] = torch.sigmoid(torch.sum(torch.cat(entity_outs[ent])))\n",
    "        return torch.stack(list(entity_outs.values())), entity_outs.keys(), preds\n",
    "\n",
    "    def predict(self, abstract, title, entity_list, ignore_big=True):\n",
    "        combined_input, mask_indices, entity_indices, entity_texts = self.fix_input(abstract, title)\n",
    "        max_len = max([e.shape[-1] for e in combined_input])\n",
    "        if ignore_big and max_len > 512:\n",
    "            return None\n",
    "        # Pad combined input\n",
    "        combined_input = torch.stack(\n",
    "            [F.pad(e, [0, max_len - e.shape[-1], 0, 0], 'constant', 0) for e in combined_input], dim=0).squeeze(dim=1)\n",
    "        if use_cuda:\n",
    "            combined_input = combined_input.cuda(gpu_device)\n",
    "        out = list()\n",
    "        bert_out = self.bert(combined_input)[0][-1]\n",
    "        for i in range(combined_input.shape[0]):\n",
    "            if len(entity_indices[i]) == 0:\n",
    "                continue\n",
    "            if len(entity_indices[i]) != 1:\n",
    "                bert_out_entities = bert_out[i, entity_indices[i], :].squeeze(dim=0)\n",
    "            else:\n",
    "                bert_out_entities = bert_out[i, entity_indices[i], :]\n",
    "            bert_out_mask = bert_out[i, mask_indices[i], :]\n",
    "            bert_out_mask = bert_out_mask.expand_as(bert_out_entities)\n",
    "            bert_out_concat = torch.cat([bert_out_entities, bert_out_mask], dim=-1)\n",
    "            out.append(self.linear2(F.relu(self.linear(bert_out_concat))))\n",
    "        # Find predictions\n",
    "        entity_texts = [e for e in entity_texts if len(e)!= 0]\n",
    "        preds = dict()\n",
    "        for et, ot in zip(entity_texts, out):\n",
    "            ot = ot.detach().cpu().numpy().tolist()\n",
    "            for e, o in zip(et, ot):\n",
    "                if e not in preds:\n",
    "                    preds[e] = 0\n",
    "                preds[e] += o[0]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca36a898-e041-4d9c-9c30-3d3380a28c53",
   "metadata": {},
   "source": [
    "#### LOAD CHECKPOINT OF PRETRAINED MODEL ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8080b-8aab-44b8-96e8-1eb637bb0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "resume_from = './biomrc/model/scibertreadersum_best_checkpoint.pth.tar' #'./bert_model_results/scibertreadersum_checkpoint.pth.tar'\n",
    "resumed = False\n",
    "if os.path.exists(resume_from):\n",
    "    checkpoint = torch.load(resume_from, map_location=torch.device('cpu')) # checkpoint = torch.load(resume_from)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_dev_acc = checkpoint['best_acc']\n",
    "    best_epoch = checkpoint['best_epoch']\n",
    "    frozen_t = checkpoint['frozen_top']\n",
    "    early_stop_counter = checkpoint['early_stop']\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_from, checkpoint['epoch']))\n",
    "    print([(e, checkpoint[e]) for e in checkpoint.keys() if e!= 'state_dict' and e!='optimizer'])\n",
    "    resumed = True\n",
    "else:\n",
    "    print('No checkpoint to load!')\n",
    "    start_epoch = 0\n",
    "    best_dev_acc = -1\n",
    "    best_epoch = -1\n",
    "    early_stop_counter = 0\n",
    "    frozen_t = True\n",
    "\n",
    "\n",
    "model = SciBertReaderSum(frozen_t)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda(gpu_device)\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d8a6d-7f41-4851-bb85-aa82ac72d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_epochs = 40\n",
    "\n",
    "# Try to resume\n",
    "if resumed:\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # Unfreeze top bert layer if we resumed with unfrozen top\n",
    "    if not model.frozen_top:\n",
    "        model.unfreeze_top(optimizer)\n",
    "        print('BERT Top Layer is Unfrozen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de6e7a-8757-474e-b4e2-f08367861e19",
   "metadata": {},
   "source": [
    "#### LOAD DATASET ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877b65f-ca1e-4013-aae0-e3be0efd7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load biomrc dataset\n",
    "dataset = load_dataset('biomrc', 'biomrc_small_B')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfeaea3-9c54-4728-ab37-028f0e550268",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'][0]['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cebf29-5ab1-4eb1-930c-982f21cd716c",
   "metadata": {},
   "source": [
    "#### SHOW EXTRACT OF DATASET ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4e2b0-1093-44aa-b49c-7a134ad12daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "end_index = 1\n",
    "for n in range(start_index, end_index):\n",
    "    print(f\"dataset['test'][{n}]:\\n{dataset['test'][n]}\\n\")\n",
    "    print(f\"abstract: \\n{dataset['test'][n]['abstract']}\")\n",
    "    print(f\"title: \\n{dataset['test'][n]['title']}\")\n",
    "    print(f\"entities_list: \\n{dataset['test'][n]['entities_list']}\\n\")\n",
    "    print(f\"answer: \\n{dataset['test'][n]['answer']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0bb7b-ad5c-4054-bc3c-5c550d3a3e52",
   "metadata": {},
   "source": [
    "#### TEST MODEL USING DATASET ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235659aa-0e41-489a-81b9-6ae2fb999ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "ab = dataset['test'][n]['abstract']\n",
    "ti = dataset['test'][n]['title']\n",
    "ents = dataset['test'][n]['entities_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a195e9-edfa-4e55-9070-d74e31e4c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OWN EDITED PREDICTION\n",
    "preds = model.predict(ab, ti, ents)\n",
    "print(f\"Prediction: {preds}\")\n",
    "# Maximum value\n",
    "best_prediction = max(preds, key=preds.get)\n",
    "\n",
    "# Best prediction\n",
    "print(f\"Best Prediction: {best_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d7de4-0106-47b1-9563-3ace18ee4ee4",
   "metadata": {},
   "source": [
    "#### PREPROCESSING OF DATA BEFORE USE MODEL ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149c1b1-6a09-475a-8a11-cb5354892bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING BEFORE RUN THE MODEL\n",
    "import scispacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "#nlp1 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2 = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "#med7 = spacy.load(\"en_core_med7_lg\")\n",
    "\n",
    "# create distinct colours for labels\n",
    "col_dict = {}\n",
    "seven_colours = ['#e6194B', '#3cb44b', '#ffe119', '#ffd8b1', '#f58231', '#f032e6', '#42d4f4']\n",
    "for label, colour in zip(med7.pipe_labels['ner'], seven_colours):\n",
    "    col_dict[label] = colour\n",
    "\n",
    "options = {'ents': med7.pipe_labels['ner'], 'colors':col_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fb469-d833-43b3-87b7-b2d3559c7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"Has the pt. ever been on caltrate plus d2 tablets before\"\n",
    "question = \"Has the patient is a 76-year-old female with a history of mitral regurgitation ever been on XXXX plus d2 tablets before\"\n",
    "context = \"The patient is a 76-year-old female with a history of mitral regurgitation, congestive heart failure, recurrent UTIs, and uterine prolapse who presented with chills and hypotension and was admitted to the Medical ICU for treatment of septic shock. Mean arterial pressures were kept above 65 with Levophed and antibiotics were changed to penicillin 3 million units IV q.4h. and gentamicin 50 mg IV q.8h. An ATEE on 10/19 showed severe mitral regurgitation with posterior leaflet calcifications and linear density concerning for endocarditis, for which a PICC line was placed on 1/19 for a six-week course of penicillin 3 million units IV q.4h. and two-week course of gentamicin 50 mg IV q.8h. until 2/25. The patient was initially treated with Levophed for her hypotension until 11/0, and was placed on Levofloxacin and Vancomycin to treat Gram-positive cocci bacteremia and UTI. She was maintained on telemetry and was found to be a normal sinus rhythm with ectopy, including short once of nonsustained ventricular tachycardia. She was started on Lopressor 12.5 mg t.i.d. on 3/18, and this was increased to 25 mg b.i.d. at discharge, with her heart rates continuing to be between the 70s and the 90s, however, with less episodes of ectopy. Aspirin was given, and Lipitor was initially held for an initial transaminitis presumed to be secondary to shock liver. She had guaiac positive stools in the medical ICU, her hematocrit was stable around 33%, and her iron studies suggested anemia of chronic disease with possibly overlying iron deficiency. She had a normal random cortisol level of 35.3, and her Hemoglobin A1c was 6.5, so she was maintained thereafter only on insulin sliding scale and rarely required any coverage. The patient was kept on Lovenox and Protonix and her DISCHARGE MEDICATIONS include Aspirin 81 mg daily, iron sulfate 325 mg daily, gentamicin sulfate 50 mg IV q.8h. until 2/25 for a two-week course, penicillin G potassium 3 million units IV q.4h. until 0/12 for a six-week course, Lopressor 25 mg b.i.d., Caltrate plus D2 tablets p.o. daily, Lipitor 10 mg daily, and Protonix 40 mg daily. She was discharged to rehabilitation at Acanmingpeerra Virg Tantblu Medical Center in order to be able to get her antibiotic therapy, and her physicians will attempt to add the ACE back onto her medical regimen for better afterload reduction as her blood pressure tolerates, and potentially they will add her back on to the Lasix as well. She will require weekly lab draws to check her electrolytes and CBC while she is on the antibiotics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b27e4-2506-413b-877c-c10c07695be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Has the patient Peter, with a history of mitral regurgitation, XXXX\"\n",
    "context = \"Peter is a patient with a history of mitral regurgitation and he has lung cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4062a12-9c37-40a2-b4c6-e90e29a4f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context1 = nlp1(context)\n",
    "context_ner = nlp2(context)\n",
    "question_ner = nlp2(question)\n",
    "#context3 = med7(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281c2bd-e6f6-405f-b78d-7d7053023403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTITY CLASSIFICATION in CONTEXT GROUPING REPEATED ONES IN ORDER TO COUNT AND ASSIGN A LABEL\n",
    "entities_context = []\n",
    "entities_texts = []\n",
    "count = 0\n",
    "for ent in context_ner.ents:\n",
    "    #print(f\"Texto completo: {context2.text[ent.start_char:ent.end_char]}\\n\")\n",
    "    if ent.text not in entities_texts:\n",
    "        entity_item = {'label': f\"@entity{count}\", 'text': ent.text, 'entity': ent.label_, 'positions': [{'start_char': ent.start_char, 'end_char': ent.end_char}]}    \n",
    "        entities_context.append(entity_item)\n",
    "        entities_texts.append(ent.text)\n",
    "        count+=1\n",
    "    else:\n",
    "        index = entities_texts.index(ent.text)\n",
    "        entities_context[index]['positions'].append({'start_char': ent.start_char, 'end_char': ent.end_char})\n",
    "\n",
    "#REPLACING IN THE CONTEXT\n",
    "for item in entities_context:\n",
    "    context = context.lower().replace(item['text'].lower(), item['label'])\n",
    "    #for pos in item['positions']:\n",
    "        #context = context[:pos['start_char']] + item['label'] + context[pos['end_char']:]\n",
    "\n",
    "\n",
    "\n",
    "#ENTITY CLASSIFICATION IN QUESTION GROUPING REPEATED ONES IN ORDER TO COUNT AND ASSIGN A LABEL\n",
    "entities_question = []\n",
    "count = 0\n",
    "for ent in question_ner.ents:\n",
    "    if ent.text not in entities_texts:\n",
    "        entity_item = {'label': f\"@entity{count}\", 'text': ent.text, 'entity': ent.label_}    \n",
    "        entities_context.append(entity_item)\n",
    "        count+=1      \n",
    "\n",
    "entity_list = []        \n",
    "#REPLACING IN THE CONTEXT\n",
    "for item in entities_context:\n",
    "    question = question.replace(item['text'].lower(), item['label'])\n",
    "    entity_list.append(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61ab01-d807-4e64-a77a-5d6e1f6b1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Context: \\n{context}\\n\\nQuestion: {question}\\n\\entity_list: {entity_list}\\n\\entities_context: {entities_context}\")\n",
    "print(\"-\" * 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8507538-348c-4e33-9ba5-527fb351bbc4",
   "metadata": {},
   "source": [
    "#### PREDICTION ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e74b47-9aa5-4e60-9195-f234c2d1d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "preds = model.predict(context, question, entity_list)\n",
    "print(f\"Prediction: {preds}\")\n",
    "# Maximum value\n",
    "best_prediction = max(preds, key=preds.get)\n",
    "\n",
    "for item in entities_context:\n",
    "    if item['label'] == best_prediction:\n",
    "        value_entity = item['text']\n",
    "        break\n",
    "    \n",
    "# Best prediction\n",
    "print(f\"\\nBEST PREDICTION: {best_prediction}, VALUE IS: {value_entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55668e-2601-4a05-a670-8f7265e9c540",
   "metadata": {},
   "source": [
    "## METRICS ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9356622-310a-40bf-985c-1db04ef55e37",
   "metadata": {},
   "source": [
    "#### LOAD DATASET TO TEST ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13a83c-9f05-4c9c-b330-409ee56e90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load biomrc_small_A dataset to test model\n",
    "dataset_small_A = load_dataset('biomrc','biomrc_small_A')\n",
    "print(dataset_small_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675140e5-6cc6-48ac-987e-ae3b3a2c3e68",
   "metadata": {},
   "source": [
    "#### METHODS AND FUNCTIONS TO GET INFORMATION ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82825556-c535-4c8c-83f7-7b9623220ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def get_true_answer(raw_true_answer):\n",
    "    true_answer_string = raw_true_answer.split(\"::\")[2].split(\"\\n\")[0].strip() #get answer\n",
    "    true_label = raw_true_answer.split(\"::\")[0].split(\"\\n\")[0].strip() #get label\n",
    "    try:\n",
    "        true_answer_list = ast.literal_eval(true_answer_string)\n",
    "        if not isinstance(true_answer_list, list):\n",
    "            print(\"The string isn't a list.\")\n",
    "        else:\n",
    "            true_answer = true_answer_list[0].lower()\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error to eval string: {e}\")\n",
    "        true_answer = None\n",
    "    return true_answer, true_label\n",
    "    \n",
    "\n",
    "def get_entities_list(list):\n",
    "    entities_list = []\n",
    "    entities_and_text_list = []\n",
    "    for item in list:\n",
    "        entity = item.split(\"::\")[0].split(\"\\n\")[0].strip() #get entity\n",
    "        text_raw = item.split(\"::\")[2].split(\"\\n\")[0].strip() #get text\n",
    "        textlist = ast.literal_eval(text_raw)\n",
    "        text = textlist[0].lower()      \n",
    "        entities_list.append(entity) \n",
    "        entities_and_text_list.append({'label':entity,'text':text})\n",
    "    return entities_list, entities_and_text_list\n",
    "\n",
    "def get_predict(context, question, entities_list, entities_and_text_list):\n",
    "    preds = model.predict(context, question, entities_list)\n",
    "    best_prediction = max(preds, key=preds.get) #Maximum value\n",
    "    for item in entities_and_text_list:\n",
    "        if item['label'] == best_prediction:\n",
    "            value_entity = item['text']\n",
    "            break\n",
    "    return best_prediction, value_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1872b-150c-4bcd-be35-3466a5b3aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "label = 'test'#'validation'#'train'\n",
    "print(f\"dataset_small_A[{label}][{n}]:\\n{dataset_small_A[label][n]}\\n\")\n",
    "print(f\"abstract: \\n{dataset_small_A[label][n]['abstract']}\")\n",
    "print(f\"title: \\n{dataset_small_A[label][n]['title']}\")\n",
    "print(f\"entities_list: \\n{dataset_small_A['test'][n]['entities_list']}\\n\")\n",
    "print(f\"answer: \\n{dataset_small_A[label][n]['answer']}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df2a01-6d16-4ec4-8ece-8349db4cf581",
   "metadata": {},
   "source": [
    "#### TEST FUNCTIONS AND METHODS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a556bbc-4033-4009-91fa-2acd6686a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_answer, true_label = get_true_answer(dataset_small_A[label][n]['answer'])\n",
    "print(f\"True answer: {true_answer}\\n\")\n",
    "print(f\"Answer: {dataset_small_A[label][n]['answer']}\\n\")\n",
    "entities_list, entities_and_text_list = get_entities_list(dataset_small_A[label][n]['entities_list'])\n",
    "print(f\"Entities list:\\n{entities_list}\\n\")\n",
    "print(f\"Entities list and texts:\\n{entities_and_text_list}\\n\")\n",
    "context = dataset_small_A[label][n]['abstract']\n",
    "question = dataset_small_A[label][n]['title']\n",
    "entities_list, entities_and_text_list = get_entities_list(dataset_small_A[label][n]['entities_list'])\n",
    "# PREDICTION\n",
    "predicted_answer, predicted_label = get_predict(context, question, entities_list, entities_and_text_list)\n",
    "print(predicted_answer)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f9223-0fda-41b2-9a32-3f7703ee00a0",
   "metadata": {},
   "source": [
    "#### GET GROUND TRUE AND PREDICTED ANSWERS OF DATASET ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70970875-1007-4ff9-b266-5eda16c4194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "count = 0\n",
    "answers_list = []\n",
    "for item in dataset_small_A[label]:\n",
    "    true_answer, true_label = get_true_answer(item['answer'])\n",
    "    if true_answer != None:    \n",
    "        entities_list, entities_and_text_list = get_entities_list(item['entities_list'])\n",
    "        predicted_label, predicted_answer = get_predict(item['abstract'], item['title'], entities_list)\n",
    "        data_dictionary = {'abstract': item['abstract'], 'title': item['title'], 'entities_list': item['entities_list'], \n",
    "                           'entities_and_text_list': entities_and_text_list, 'true_answer': true_answer, 'true_label': true_label, \n",
    "                           'predicted_answer': predicted_answer, 'predicted_label': predicted_label}\n",
    "        answers_list.append(data_dictionary)\n",
    "        clear_output(wait=True)  \n",
    "        print(f\"Length of dataset_small_A[{label}]: {len(dataset_small_A[label])}\")\n",
    "        print(f\"Count: {count}\")\n",
    "        count += 1\n",
    "\n",
    "datos = answers_list\n",
    "\n",
    "#WRITE NEW FILE WITH VALIDATE DATA            \n",
    "file_name = \"biomrc/output/biomrc_small_A_scibert_comparison_data.json\"            \n",
    "with open(file_name, 'w') as archivo_json:\n",
    "    json.dump(datos, archivo_json)\n",
    "\n",
    "print(f\"Length {len(datos)} of comparison results.\")\n",
    "print(f\"Saved information in {file_name}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0cd08-c9cc-46ad-a922-cc643015cf1a",
   "metadata": {},
   "source": [
    "#### READ FILE WITH RESULTS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee7a76-5276-4e78-bfbd-191e950a5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ FILE\n",
    "import json\n",
    "file_name = \"biomrc/output/biomrc_small_A_scibert_comparison_data.json\"   \n",
    "\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    biomrc_small_A_scibert_comparison_data = json.load(archivo_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2250f92-7f90-45fb-9b63-2ec04fd168c8",
   "metadata": {},
   "source": [
    "#### GET METRICS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc0ff6-a44a-4fd5-a717-d83b964f5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "#GET predicted_answers AND true_answers LISTS\n",
    "predicted_answers = []\n",
    "true_answers = []\n",
    "for item in biomrc_small_A_scibert_comparison_data:\n",
    "    predicted_answers.append(item['predicted_answer'])\n",
    "    true_answers.append(item['true_answer'])\n",
    "    \n",
    "# RECALL AND F1 SCORE\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_answers, predicted_answers, average='weighted', zero_division=0.0)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f31704-6886-47c9-9c8e-5092cf51fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXACT MATCH\n",
    "import re\n",
    "#Exact Match\n",
    "em = []\n",
    "for result in biomrc_small_A_scibert_comparison_data:\n",
    "    pred = re.sub('[^0-9a-z ]', '', result['predicted_answer'].lower())\n",
    "    true = re.sub('[^0-9a-z ]', '', result['true_answer'].lower())\n",
    "    if pred == true:\n",
    "        em.append(1)\n",
    "    else:\n",
    "        em.append(0)\n",
    "\n",
    "average = sum(em)/len(em)\n",
    "print(\"Exact match:\")\n",
    "print(average)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e25740-ee91-4fa4-8ed9-098c545c3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROUGE\n",
    "from rouge import Rouge\n",
    "model_out = [ans['predicted_answer'] for ans in biomrc_small_A_scibert_comparison_data]\n",
    "reference = [ans['true_answer'] for ans in biomrc_small_A_scibert_comparison_data]\n",
    "\n",
    "clean = re.compile('(?i)[^0-9a-z ]')\n",
    "model_out = [clean.sub('', text.lower()) for text in model_out]\n",
    "reference = [clean.sub('', text.lower()) for text in reference]\n",
    "\n",
    "for i, o in enumerate(model_out):\n",
    "    if o == \"\":\n",
    "        model_out[i] = \"vacio\"\n",
    "#print(\"Model Output:\", model_out)\n",
    "#print(\"Type of Model Output:\", type(model_out))\n",
    "#print(len(model_out))\n",
    "#print(\"\\nReference:\", reference)\n",
    "#print(\"Type of Reference:\", type(reference))\n",
    "#print(len(reference))\n",
    "\n",
    "rouge = Rouge()\n",
    "#print(\"Hola\")\n",
    "result_rouge = rouge.get_scores(model_out, reference)\n",
    "#print(\"Hola2\")\n",
    "result_rouge_avg = rouge.get_scores(model_out, reference, avg=true)\n",
    "scores = rouge.get_scores(model_out, reference)\n",
    "#print(\"ROUGE:\")\n",
    "#print(result_rouge)\n",
    "print(\"ROUGE AVERAGE:\")\n",
    "print(result_rouge_avg)\n",
    "indice = 8\n",
    "print(f\"rouge[{indice}]:\")\n",
    "print(model_out[indice], '|', reference[indice], '|', scores[indice]['rouge-1']['f'])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4606f-4155-45b1-8c14-754cb888d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1-SCORE\n",
    "import numpy as np\n",
    "#F1-Score\n",
    "#RECALL = count(match reference y model gram n)/count(reference gram n)\n",
    "#recall = [print(x) for x in [1,2,3,4,5,6] if x < 5]\n",
    "list_ref = []\n",
    "print(\"F1-SCORE:\\n\")\n",
    "\n",
    "#print(\"Reference:\")\n",
    "for ref in reference:\n",
    "    split_ref = ref.split()\n",
    "    list_ref.append(split_ref)\n",
    "\n",
    "#print(list_ref)\n",
    "\n",
    "list_model = []\n",
    "#print(\"\\nModel:\")\n",
    "for model in model_out:\n",
    "    split_model = model.split()\n",
    "    list_model.append(split_model)\n",
    "    \n",
    "#print(list_model)\n",
    "#print(list_model[0])\n",
    "i = 0\n",
    "list_match = []\n",
    "for phrase in list_ref:\n",
    "    count = 0\n",
    "    for word in phrase:\n",
    "        if word in list_model[i]:\n",
    "            count = count + 1\n",
    "    list_match.append(count)\n",
    "    i = i+1\n",
    "\n",
    "list_countref = []\n",
    "list_countmodel = []\n",
    "#count ngram\n",
    "for phrase in list_ref:\n",
    "    list_countref.append(len(phrase))\n",
    "for phrase in list_model:\n",
    "    list_countmodel.append(len(phrase))\n",
    "#print(\"\\nCount words ref:\")\n",
    "#print(list_countref)\n",
    "#print(\"\\nCount words model:\")\n",
    "#print(list_countmodel)\n",
    "#print(\"\\nCount words match:\")\n",
    "#print(list_match)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "match = np.array(list_match)\n",
    "count_ref = np.array(list_countref)\n",
    "count_model = np.array(list_countmodel)\n",
    "\n",
    "array_recall = match/count_ref\n",
    "array_precision = match/count_model\n",
    "print(\"\\nRecall array:\")\n",
    "print(array_recall)\n",
    "print(\"\\nPrecision array:\")\n",
    "print(array_precision)\n",
    "\n",
    "num = 2*array_precision*array_recall\n",
    "den = array_precision + array_recall\n",
    "\n",
    "f1_score = num/den\n",
    "print(\"\\nF1-Score:\")\n",
    "print(f1_score)\n",
    "#print(type(f1_score))\n",
    "\n",
    "# Reemplazar los valores nan por 0.0\n",
    "my_array_no_nan = np.nan_to_num(f1_score, nan=0.0)\n",
    "#print(type(my_array_no_nan))\n",
    "# Convertir el numpy array a una lista\n",
    "f1_score_list_biomrc = my_array_no_nan.tolist()\n",
    "#print(type(f1_score_list_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b723bdd-6d70-4bcf-a620-a7993cc1f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [score for score in f1_score_list_biomrc if score <= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c8240-f860-4f1b-aa26-dfb18c065c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "list_aux = filtered_list #f1_score_list_biomrc\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(list_aux, bins=30, color='green', edgecolor='black')  # Experimenta con el número de bins\n",
    "# Añadir etiquetas y título\n",
    "plt.xlabel('F1-Score')\n",
    "plt.ylabel('Frecuency')\n",
    "plt.title('BIOMRC scibert_scivocab_uncased - F1-Score Distribution')\n",
    "\n",
    "# Show\n",
    "plt.tight_layout()\n",
    "plt.savefig('biomrc/figures/histogram_biomrc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f24c1-7f3f-43ff-8393-e95f31595c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "list_aux = f1_score_list_biomrc\n",
    "# Ranges\n",
    "bins = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(list_aux, bins=bins, color='green', edgecolor='black') \n",
    "\n",
    "# Labels and titles\n",
    "plt.xlabel('F1-Score')\n",
    "plt.ylabel('Frecuency')\n",
    "plt.title('BIOMRC scibert_scivocab_uncased - F1-Score Distribution in ranges')\n",
    "\n",
    "# xticks\n",
    "plt.xticks(np.arange(0, 1.25, 0.25))\n",
    "\n",
    "# Show\n",
    "plt.tight_layout()\n",
    "plt.savefig('biomrc/figures/histogram_biomrc_ranges.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ddc83b-0349-4cf4-b5ef-5d26b5b571e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "list_aux = f1_score_list_biomrc\n",
    "\n",
    "# Ranges\n",
    "bins = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Frequency\n",
    "hist, _ = np.histogram(list_aux, bins=bins)\n",
    "\n",
    "# Labels\n",
    "labels = [f'{bins[i]:.2f}-{bins[i+1]:.2f}' for i in range(len(bins)-1)]\n",
    "\n",
    "# Pie Chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(hist, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#FF9999', '#66B2FF', '#99FF99', '#FFCC99'])\n",
    "plt.title('BIOMRC scibert_scivocab_uncased - Pie chart of F1-Score Distribution in ranges')\n",
    "\n",
    "# Show\n",
    "plt.savefig('biomrc/figures/piechart_biomrc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef6af1-ac08-4d77-b161-cb05959a1427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

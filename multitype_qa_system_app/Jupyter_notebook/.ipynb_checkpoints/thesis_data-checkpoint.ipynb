{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPLOAD FILE\n",
    "import json\n",
    "with open(\"data.json\") as archivo:\n",
    "    dictionary = json.load(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"Length of 'dictionary['data']': \"+str(len(dictionary[\"data\"])))\n",
    "#print(\"Length of 'dictionary['data'][0]': \"+str(len(dictionary[\"data\"][0])))\n",
    "#print(\"Length of 'dictionary['data'][0]['paragraphs']': \"+str(len(dictionary[\"data\"][0][\"paragraphs\"])))\n",
    "#print(\"Length of 'dictionary['data'][0]['paragraphs'][0]': \"+str(len(dictionary[\"data\"][0][\"paragraphs\"][0])))\n",
    "#print(\"Length of 'dictionary['data'][0]['paragraphs'][0]['context']': \"+str(len(dictionary[\"data\"][0][\"paragraphs\"][0][\"context\"])))\n",
    "#print(\"Length of 'dictionary['data'][0]['paragraphs'][0]['qas']': \"+str(len(dictionary[\"data\"][0][\"paragraphs\"][0][\"qas\"])))\n",
    "#print(\"Length of 'dictionary['data'][0]['paragraphs'][0]['qas'][0]': \"+str(len(dictionary[\"data\"][0][\"paragraphs\"][0][\"qas\"][0])))\n",
    "#print(\"Length of 'dictionary['data'][0]['paragraphs'][0]['qas'][0]['answers']': \"+str(len(dictionary[\"data\"][0][\"paragraphs\"][0][\"qas\"][0][\"answers\"])))\n",
    "#print(\"Length of 'dictionary['data'][0]['paragraphs'][0]['qas'][0]['id']': \"+str(len(dictionary[\"data\"][0][\"paragraphs\"][0][\"qas\"][0][\"id\"])))\n",
    "#print(\"Length of 'dictionary['data'][0]['paragraphs'][0]['qas'][0]['question']': \"+str(len(dictionary[\"data\"][0][\"paragraphs\"][0][\"qas\"][0][\"question\"]))+\"\\n\")\n",
    "\n",
    "i = 0 #Varia el parrafo\n",
    "j = 0 #Varia la respuesta\n",
    "k = 0 #Varia la pregunta con la misma respuesta\n",
    "l = 0 #It is Only 0\n",
    "#print(\"PARAGRAPH \"+str(i)+\":\\n\")\n",
    "#print(\"ID:\"+dictionary[\"data\"][0][\"paragraphs\"][i][\"note_id\"]+\"\\n\")\n",
    "#print(\"CONTEXT \"+str(i)+\":\\n\"+''.join(dictionary[\"data\"][0][\"paragraphs\"][i][\"context\"]))\n",
    "#print(\"Question: \"+dictionary[\"data\"][0][\"paragraphs\"][i][\"qas\"][j][\"question\"][k]+\"\\n\")\n",
    "#print(\"Answer true: \"+dictionary[\"data\"][0][\"paragraphs\"][i][\"qas\"][j][\"answers\"][l][\"evidence\"]+\"\\n\")\n",
    "#print(\"=\" * 50)\n",
    "\n",
    "# Se simplifica la data que se va a usar y se guarda en una lista \"data_base\"\n",
    "\n",
    "data_base = [] #Contiene parrafos --- data.append(data_paragraph)\n",
    "for paragraph in dictionary[\"data\"][0][\"paragraphs\"]:#Varia los parrafos\n",
    "    data_paragraph = {\n",
    "    'context': '',\n",
    "    'joined_context':'',\n",
    "    'note_id': '',\n",
    "        'qas': [] #Contiene preguntas con una respuesta --- qas.append(qas_item)\n",
    "    }    \n",
    "    \n",
    "    #Se anexan el id y contexto a la lista de data\n",
    "    data_paragraph[\"note_id\"] = paragraph[\"note_id\"]\n",
    "    data_paragraph[\"context\"] = paragraph[\"context\"]\n",
    "    data_paragraph[\"joined_context\"] = ''.join(paragraph[\"context\"])\n",
    "    \n",
    "    #print(\"ID:\"+paragraph[\"note_id\"]+\"\\n\")\n",
    "    #print(\"CONTEXT:\\n\"+''.join(paragraph[\"context\"]))\n",
    "    for item in paragraph[\"qas\"]:#Varia respuesta\n",
    "        qas_item = {\n",
    "        'true_answer': '',\n",
    "        'questions': [] #Contiene preguntas --- questions.append(question) --- question es un texto\n",
    "        }  \n",
    "        \n",
    "        #Se anexa la respuesta correspondiente al ciclo de preguntas\n",
    "        qas_item[\"true_answer\"] = ''.join(item[\"answers\"][0][\"evidence\"])\n",
    "        \n",
    "        #print(\"*\" * 50)\n",
    "        #print(\"ANSWER TRUE:\"+''.join(item[\"answers\"][0][\"evidence\"]))\n",
    "        \n",
    "        for question in item[\"question\"]:#Varia las preguntas con la misma respuesta\n",
    "            \n",
    "            #Se anexa pregunta a la lista de preguntas correspondiente a una respuesta\n",
    "            qas_item[\"questions\"].append(question)\n",
    "            \n",
    "            #print(\"QUESTION:\"+question+\"\\n\")\n",
    "            #print(\"=\" * 50)\n",
    "            #break \n",
    "        data_paragraph[\"qas\"].append(qas_item) \n",
    "        #break\n",
    "    data_base.append(data_paragraph)\n",
    "    #break\n",
    "\n",
    "    \n",
    "# Visualizar data guardada en la lista \"data_base\": Es la data original pero simplificada a las necesidades\n",
    "\n",
    "#for paragraph in data_base:\n",
    "#    print(\"ID:\"+paragraph[\"note_id\"]+\"\\n\")\n",
    "#    print(\"CONTEXT:\\n\"+paragraph[\"joined_context\"])\n",
    "#    for qas_item in paragraph[\"qas\"]:\n",
    "#        print(\"*\" * 50)\n",
    "#        print(\"ANSWER TRUE:\"+qas_item[\"true_answer\"])\n",
    "#        for question in qas_item[\"questions\"]:\n",
    "#            print(\"QUESTION:\"+question+\"\\n\")\n",
    "#            print(\"=\" * 50)\n",
    "#    break # Para visualizar solo el primer parrafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_base[0][\"qas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import math\n",
    "openai.api_key = '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' #CLAVE DE API\n",
    "\n",
    "#SIMPLIFICACION DE CANTIDAD DE RESPUESTAS\n",
    "data_base_ans_simplify = [] #Contiene parrafos --- data.append(data_paragraph)\n",
    "count = 249# count = 0 #Falla en: 37, 38, 46, 91, 116, 214, 222, 247, 248  \n",
    "for paragraph in data_base[248:]:#Varia los parrafos\n",
    "    data_paragraph = {\n",
    "    'count':count,\n",
    "    'context': '',\n",
    "    'joined_context':'',\n",
    "    'summary':'',\n",
    "    'note_id': '',\n",
    "        'qas': [] #Contiene preguntas con una respuesta --- qas.append(qas_item)\n",
    "    }    \n",
    "    \n",
    "    #Se anexan el id y contexto a la lista de data\n",
    "    data_paragraph[\"note_id\"] = paragraph[\"note_id\"]\n",
    "    data_paragraph[\"context\"] = paragraph[\"context\"] #Texto en array\n",
    "    data_paragraph[\"joined_context\"] = paragraph[\"joined_context\"] #Texto unido\n",
    "      \n",
    "    answers = '' #Inicializacion de recopilador de respuestas unicas sobre un texto\n",
    "    o = 1 #Inicializacion de contador    \n",
    "    for item in paragraph[\"qas\"]:#Varia respuesta\n",
    "        qas_item = {\n",
    "        'true_answer': '',\n",
    "        'questions': [] #Contiene preguntas --- questions.append(question) --- question es un texto\n",
    "        }  \n",
    "        \n",
    "        if not any(item[\"true_answer\"] == qas_item_aux[\"true_answer\"] for qas_item_aux in data_paragraph[\"qas\"]):\n",
    "            #Se anexa la respuesta correspondiente al ciclo de preguntas\n",
    "            qas_item[\"true_answer\"] = item[\"true_answer\"]\n",
    "            \n",
    "            answers = answers+\"\\n\"+f\"{o}. \"+item[\"true_answer\"] #Recopilacion de respuestas unicas sobre un texto\n",
    "            o+=1 #Incremento de contador              \n",
    "            \n",
    "            for question in item[\"questions\"]:#Varia las preguntas con la misma respuesta                  \n",
    "                #Se anexa pregunta a la lista de preguntas correspondiente a una respuesta\n",
    "                qas_item[\"questions\"].append(question)\n",
    "            data_paragraph[\"qas\"].append(qas_item) \n",
    "            \n",
    "        elif any(item[\"true_answer\"] == qas_item_aux[\"true_answer\"] for qas_item_aux in data_paragraph[\"qas\"]):  \n",
    "            true_answer_to_find = item[\"true_answer\"]\n",
    "            index = None  # Inicializamos index como None en caso de que no se encuentre una coincidencia\n",
    "\n",
    "            for i, qas_item_aux in enumerate(data_paragraph[\"qas\"]):\n",
    "                if qas_item_aux[\"true_answer\"] == true_answer_to_find:\n",
    "                    index = i\n",
    "                    break  # Terminamos el bucle cuando encontramos la igualdad\n",
    "            \n",
    "            for question in item[\"questions\"]: \n",
    "                data_paragraph[\"qas\"][index][\"questions\"].append(question)\n",
    "    \n",
    "    #DETECTAR QUE TEXTOS SON MUY LARGOS Y CUALES SON ADECUADOS Y CLASIFICAR EN DOS ARRAYS PARA LUEGO RESUMIR\n",
    "    #CODIGO\n",
    "    \n",
    "    print(\"VAMOS POR: \"+str(count))\n",
    "    prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "    Medical Report:\\n {paragraph[\"joined_context\"]} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"      \n",
    "    # Divide el prompt en tokens separados por espacio\n",
    "    tokens = prompt.split()\n",
    "    # Calcula la cantidad de tokens\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    #Calcula la cantidad de token en el texto\n",
    "    context_length = len(paragraph[\"context\"])\n",
    "    token_context = len(paragraph[\"joined_context\"].split())\n",
    "    \n",
    "    # Verifica si el token_count excede el límite\n",
    "    if token_count > 4097:\n",
    "        print(f\"El prompt excede el límite de tokens 4097:  tiene {token_count} tokens de prompt, {context_length} tamaño de paragraph['context'] y {token_context} tokens de context.\")\n",
    "    elif token_count > 3000:\n",
    "        print(f\"El prompt tiene mas de 3000 tokens: tiene {token_count} de prompt, {context_length} tamaño de paragraph['context'] y {token_context} tokens de context.\")\n",
    "    else:\n",
    "        print(f\"El prompt tiene {token_count} de prompt, {context_length} tamaño de paragraph['context'] y {token_context} tokens de context.\")    \n",
    "    \n",
    "    lista = paragraph[\"context\"]\n",
    "    \n",
    "    divisor = 100 #parametro de divisibilidad para determinar en cuantas partes se dividira el reporte\n",
    "    max_tokens = 700 #Parametro de maxima cantidad de tokens\n",
    "    \n",
    "    if token_count > 4000:\n",
    "        divisor = 25 #parametro de divisibilidad para determinar en cuantas partes se dividira el reporte\n",
    "        max_tokens = 400 #Parametro de maxima cantidad de tokens\n",
    "    \n",
    "    particiones = []\n",
    "    \n",
    "    qty = math.ceil(context_length / divisor) #Cantidad de partes que se va a dividir el reporte\n",
    "    print(f\"Cantidad de partes: {qty}\")\n",
    "    \n",
    "    #if qty >= 4 and count > 54:#55 y 188\n",
    "    length = len(lista) // qty #Longitud de separacion entre partes\n",
    "    print(f\"Longitud de partes: {length}\")\n",
    "    print(\"Particiones: \")\n",
    "    c = 1\n",
    "    for i in range(0, len(lista), length):\n",
    "        if c < qty:\n",
    "            print(f\"Particion {c} desde {i} hasta {i + length}\")\n",
    "            particion = ''.join(lista[i:i + length])\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {particion} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\" \n",
    "            response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=max_tokens,temperature=0.6) # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "            summary = response.choices[0].text.strip() # Extrae la respuesta predicha del resultado de la API\n",
    "            particiones.append(summary) #  particiones.append(summary) se hacen dentro del if para que no hayan falsos guardados           \n",
    "        elif c == qty:\n",
    "            print(f\"Particion {c} desde {i} hasta {len(lista)}\")\n",
    "            particion = ''.join(lista[i:])\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {particion} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"            \n",
    "            response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=max_tokens,temperature=0.6) # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "            summary = response.choices[0].text.strip() # Extrae la respuesta predicha del resultado de la API\n",
    "            particiones.append(summary) #  particiones.append(summary) se hacen dentro del if para que no hayan falsos guardados               \n",
    "\n",
    "        c+=1\n",
    "\n",
    "    if len(particiones) == 1:\n",
    "        print(\"Resumen de la unica parte\")\n",
    "        data_paragraph[\"summary\"] = particiones[0]\n",
    "    else:     \n",
    "        summaries = []\n",
    "        j = 0\n",
    "        for i in range(0, len(particiones)):             \n",
    "            prompt = ''\n",
    "            if i == 0:\n",
    "                print(f\"Resumen {i+1}\")\n",
    "                prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "                Medical Report:\\n {particiones[i]+particiones[i+1]} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"            \n",
    "            else:\n",
    "                print(f\"Resumen {i+1}\")\n",
    "                prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "                Medical Report:\\n {summaries[j]+particiones[i+1]} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"                \n",
    "                j+=1\n",
    "\n",
    "            if i+1 == len(particiones)-1: \n",
    "                if token_count > 4000:               \n",
    "                    response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=400,temperature=0.6) # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "                    summary = response.choices[0].text.strip() # Extrae la respuesta predicha del resultado de la API\n",
    "                    summaries.append(summary) \n",
    "                else:\n",
    "                    response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=1000,temperature=0.6) # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "                    summary = response.choices[0].text.strip() # Extrae la respuesta predicha del resultado de la API\n",
    "                    summaries.append(summary)                     \n",
    "                break\n",
    "            else:              \n",
    "                response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=max_tokens,temperature=0.6) # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "                summary = response.choices[0].text.strip() # Extrae la respuesta predicha del resultado de la API\n",
    "                summaries.append(summary)\n",
    "\n",
    "        print(f\"Guardado data_paragraph['summary'] = summaries[{len(summaries)-1}]\")\n",
    "        data_paragraph[\"summary\"] = summaries[len(summaries)-1]\n",
    "\n",
    "\n",
    "    count+=1\n",
    "    data_base_ans_simplify.append(data_paragraph)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ESCRIBIR ARCHIVOS POR TROZOS CON DATA RESUMEN POR CHATGPT DE LOS TEXTOS\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON donde se guardarán los datos\n",
    "nombre_archivo = \"data_improved_1_249a262.json\"\n",
    "\n",
    "# Abrir el archivo en modo escritura\n",
    "with open(nombre_archivo, 'w') as archivo_json:\n",
    "    # Escribir el contenido del diccionario en el archivo JSON\n",
    "    json.dump(data_base_ans_simplify, archivo_json)\n",
    "\n",
    "print(f\"Se ha guardado la información en {nombre_archivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CONSOLIDAR DATA DE LOS DISTINTOS TROZOS\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name1 = \"data_improved_1.json\"\n",
    "file_name2 = \"data_improved_1_39a45.json\"\n",
    "file_name3 = \"data_improved_1_47a90.json\"\n",
    "file_name4 = \"data_improved_1_92a115.json\"\n",
    "file_name5 = \"data_improved_1_117a213.json\"\n",
    "file_name6 = \"data_improved_1_215a221.json\"\n",
    "file_name7 = \"data_improved_1_223a246.json\"\n",
    "file_name8 = \"data_improved_1_249a262.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name1, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos1 = json.load(archivo_json)\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name2, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos2 = json.load(archivo_json)    \n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name3, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos3 = json.load(archivo_json)\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name4, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos4 = json.load(archivo_json)\n",
    "    # Abrir el archivo en modo lectura\n",
    "with open(file_name5, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos5 = json.load(archivo_json)\n",
    "    # Abrir el archivo en modo lectura\n",
    "with open(file_name6, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos6 = json.load(archivo_json)\n",
    "    # Abrir el archivo en modo lectura\n",
    "with open(file_name7, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos7 = json.load(archivo_json)\n",
    "    # Abrir el archivo en modo lectura\n",
    "with open(file_name8, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos8 = json.load(archivo_json)\n",
    "    \n",
    "    \n",
    "for datos in datos2:\n",
    "    datos1.append(datos)\n",
    "for datos in datos3:\n",
    "    datos1.append(datos)\n",
    "for datos in datos4:\n",
    "    datos1.append(datos)\n",
    "for datos in datos5:\n",
    "    datos1.append(datos)\n",
    "for datos in datos6:\n",
    "    datos1.append(datos)\n",
    "for datos in datos7:\n",
    "    datos1.append(datos)\n",
    "for datos in datos8:\n",
    "    datos1.append(datos)    \n",
    "# Ahora, puedes trabajar con los datos como un diccionario (o una lista, según lo que haya en el archivo)\n",
    "print(\"Datos leídos del archivo JSON:\")\n",
    "print(f\"Longitud de datos1: {len(datos1)}\")\n",
    "\n",
    "#GUARDAR EN UN SOLO ARCHIVO\n",
    "# Nombre del archivo JSON donde se guardarán los datos\n",
    "nombre_archivo = \"data_improved_base.json\"\n",
    "\n",
    "# Abrir el archivo en modo escritura\n",
    "with open(nombre_archivo, 'w') as archivo_json:\n",
    "    # Escribir el contenido del diccionario en el archivo JSON\n",
    "    json.dump(datos1, archivo_json)\n",
    "\n",
    "print(f\"Se ha guardado la información en {nombre_archivo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LEER DATA Y VALIDAR RESPUESTAS\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos = json.load(archivo_json)\n",
    "    \n",
    "for data in datos:\n",
    "    if data['count'] == 17: #==\n",
    "        print(f\"\\nCount: {data['count']}\")\n",
    "        #print(f\"Original Report:\\n {data['joined_context']}\\n\")\n",
    "        print(f\"Summary:\\n {data['summary']}\\n\")\n",
    "        print(\"Respuestas:\\n\")\n",
    "        o = 1\n",
    "        for qas in data['qas']:\n",
    "            print(f\"{o}) {qas['true_answer']}\")#print(f\"{o}) {qas['true_answer']}\")\n",
    "            o+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUITAMOS TODAS LAS RESPUESTAS QUE TENGAN SALTO DE LINEA PARA QUEDARNOS CON RESPUESTAS CONTINUAS Y QUE SEAN UN SOLO EXTRACTO\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos = json.load(archivo_json)\n",
    "\n",
    "for data in datos:\n",
    "    if data['count'] >= 17:\n",
    "        print(f\"\\nCount: {data['count']}\")\n",
    "        print(\"Respuestas:\\n\")\n",
    "        # QUITAMOS TODAS LAS RESPUESTAS QUE TENGAN SALTO DE LINEA PARA QUEDARNOS CON RESPUESTAS CONTINUAS Y QUE SEAN UN SOLO EXTRACTO\n",
    "        data['qas'] = [qas for qas in data['qas'] if len(qas['true_answer'].split(\"\\n\")) <= 2]\n",
    "        # Ajustamos cada respuesta en una sola línea\n",
    "        #data['qas'] = [qas.update({'true_answer': qas['true_answer'].split(\"\\n\")[0]}) for qas in data['qas']]\n",
    "        \"\"\"\n",
    "        for qas in data['qas']:\n",
    "            qas['true_answer'] = qas['true_answer'].split('\\n')[0]\n",
    "            print(qas['true_answer'])\"\"\"\n",
    "\n",
    "#GUARDAR EN UN SOLO ARCHIVO\n",
    "# Nombre del archivo JSON donde se guardarán los datos\n",
    "nombre_archivo = \"data_improved2.json\"\n",
    "\n",
    "# Abrir el archivo en modo escritura\n",
    "with open(nombre_archivo, 'w') as archivo_json:\n",
    "    # Escribir el contenido del diccionario en el archivo JSON\n",
    "    json.dump(datos, archivo_json)\n",
    "\n",
    "print(f\"Se ha guardado la información en {nombre_archivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESCRIBIR NUMEROS DEL CONTADOR\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "#file_name = \"data_improved2.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos = json.load(archivo_json)\n",
    "\n",
    "n = 1    \n",
    "for data in datos:\n",
    "    data['count'] = n\n",
    "    n+=1\n",
    "\n",
    "# Abrir el archivo en modo escritura\n",
    "#with open(nombre_archivo, 'w') as archivo_json:\n",
    "#    # Escribir el contenido del diccionario en el archivo JSON\n",
    "#    json.dump(datos, archivo_json)\n",
    "\n",
    "#print(f\"Se ha guardado la información en {nombre_archivo}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LEER DATA Y VALIDAR RESPUESTAS\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved2.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos = json.load(archivo_json)\n",
    "\n",
    "limit_inf = 253\n",
    "limit_sup = 253#limit_inf\n",
    "for data in datos:\n",
    "    if data['count'] >= limit_inf and data['count'] <= limit_sup: #==\n",
    "        print(f\"\\nCount: {data['count']}\")\n",
    "        #print(f\"Original Report:\\n {data['joined_context']}\\n\")\n",
    "        print(f\"Summary:\\n {data['summary']}\\n\")\n",
    "        print(\"Respuestas:\\n\")\n",
    "        o = 1\n",
    "        for qas in data['qas']:\n",
    "            print(f\"{qas['true_answer']}\")#print(f\"{o}) {qas['true_answer']}\")\n",
    "            o+=1\n",
    "#HAY QUE VOLVER A ASIGNAR NUMEROS Y FILTRAR RESPUESTAS REPETIDAS 26-10-2023 (187 BORRADO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESCRIBIR NUMEROS DEL CONTADOR LUEGO DE HABER ARREGLADO\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved2.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos = json.load(archivo_json)\n",
    "\n",
    "n = 1    \n",
    "for data in datos:\n",
    "    data['count'] = n\n",
    "    n+=1\n",
    "\n",
    "# Abrir el archivo en modo escritura\n",
    "with open(file_name, 'w') as archivo_json:\n",
    "    # Escribir el contenido del diccionario en el archivo JSON\n",
    "    json.dump(datos, archivo_json)\n",
    "\n",
    "print(f\"Se escribieron {len(datos)} textos medicos.\")\n",
    "print(f\"Se ha guardado la información en {file_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#REORGANIZAR RESPUESTAS REPETIDAS Y AGRUPAR LAS PREGUNTAS\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved2.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos = json.load(archivo_json)\n",
    "\n",
    "print(f\"Longitud: {len(datos)}\")\n",
    "for data in datos:\n",
    "    print(f\"\\nCount: {data['count']}\")\n",
    "\n",
    "    answers_list = []\n",
    "    for qas in data[\"qas\"]:\n",
    "        print(f\"{qas['true_answer']}\")\n",
    "        answers_list.append(qas[\"true_answer\"])\n",
    "\n",
    "    # Crear un diccionario para realizar un seguimiento de los índices de los valores\n",
    "    index_dict = defaultdict(list)\n",
    "\n",
    "    # Recopilar los índices de todos los valores\n",
    "    for index, answer in enumerate(answers_list):\n",
    "        index_dict[answer].append(index)\n",
    "\n",
    "    # Encontrar grupos de índices repetidos para valores que se repiten\n",
    "    repeated_groups = [(value, indices) for value, indices in index_dict.items() if len(indices) > 1]\n",
    "\n",
    "    # repeated_groups ahora contiene grupos de valores y sus índices repetidos\n",
    "    # Mostrar el valor repetido junto con sus índices\n",
    "    if repeated_groups:\n",
    "        print(\"\\nIndices repetidos:\")\n",
    "        for value, indices in repeated_groups:\n",
    "            print(f\"El valor '{value}' se repite en los índices: {', '.join(map(str, indices))}\")   \n",
    "            #print(indices)\n",
    "            for ind in indices:\n",
    "                #print(f\"Indice: {ind}\")\n",
    "                #print(f\"Indice fijo: {indices[len(indices)-1]}\")\n",
    "                if ind != indices[len(indices)-1]:\n",
    "                    #print(f\"Indice dentro del if: {ind}\")\n",
    "                    for question in data[\"qas\"][ind][\"questions\"]:\n",
    "                        data[\"qas\"][indices[len(indices)-1]][\"questions\"].append(question) \n",
    "    \n",
    "    #CREANDO LISTA NUEVA SIN REPETIDOS\n",
    "    new_qas_list = []\n",
    "    \n",
    "    if repeated_groups:\n",
    "        print(\"\\nIndices repetidos:\")\n",
    "        get_indices_to_remove = []\n",
    "        for value, indices in repeated_groups:\n",
    "            print(f\"El valor '{value}' se repite en los índices: {', '.join(map(str, indices))}\")   \n",
    "            print(indices)   \n",
    "            get_indices_to_remove.append(indices[:-1]) \n",
    "            indices_to_remove = [item for sublist in get_indices_to_remove for item in sublist]\n",
    "        \n",
    "        print(\"Indices a eliminar:\")\n",
    "        print(indices_to_remove)\n",
    "        \n",
    "        c = 0\n",
    "        for qas in data[\"qas\"]:\n",
    "            if c not in indices_to_remove:\n",
    "                new_qas_list.append(qas)\n",
    "            c+=1\n",
    "        \n",
    "        data[\"qas\"] = new_qas_list\n",
    "        \n",
    "        #for qas in data[\"qas\"]:\n",
    "            #print(f\"{qas['true_answer']}\")   \n",
    "\n",
    "#ESCRIBIENDO NUEVO ARCHIVO SIN REPETIDOS            \n",
    "file_name2 = \"data_improved3.json\"            \n",
    "# Abrir el archivo en modo escritura\n",
    "with open(file_name2, 'w') as archivo_json:\n",
    "    # Escribir el contenido del diccionario en el archivo JSON\n",
    "    json.dump(datos, archivo_json)\n",
    "\n",
    "print(f\"Se escribieron {len(datos)} textos medicos.\")\n",
    "print(f\"Se ha guardado la información en {file_name2}\")                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LEER ARCHIVO SIN REPETIDOS Y ESCRIBIR INICIO Y FIN DE RESPUESTA\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved4.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos = json.load(archivo_json)\n",
    "\n",
    "for data in datos: \n",
    "    print(f\"\\nCount: {data['count']}\")\n",
    "    #print(f\"Original Report:\\n {data['joined_context']}\\n\")\n",
    "    print(f\"Summary:\\n {data['summary']}\\n\")\n",
    "    texto = data['summary']\n",
    "    print(\"Respuestas:\\n\")\n",
    "    o = 1\n",
    "    for qas in data['qas']:\n",
    "        print(f\"{qas['true_answer']}\")#print(f\"{o}) {qas['true_answer']}\")\n",
    "        subcadena = qas['true_answer']\n",
    "        inicio = texto.find(subcadena)\n",
    "        if inicio != -1:\n",
    "            fin = inicio + len(subcadena)\n",
    "            qas['start'] = inicio\n",
    "            qas['end'] = fin\n",
    "            print(\"Inicio:\", inicio)\n",
    "            print(\"Fin:\", fin)\n",
    "        else:\n",
    "            print(\"La subcadena no se encontró en el texto.\")            \n",
    "        o+=1\n",
    "\n",
    "#ESCRIBIENDO NUEVO ARCHIVO CON INICIO Y FIN DE LA RESPUESTA            \n",
    "file_name2 = \"data_improved4.json\"            \n",
    "# Abrir el archivo en modo escritura\n",
    "with open(file_name2, 'w') as archivo_json:\n",
    "    # Escribir el contenido del diccionario en el archivo JSON\n",
    "    json.dump(datos, archivo_json)\n",
    "\n",
    "print(f\"Se escribieron {len(datos)} textos medicos.\")\n",
    "print(f\"Se ha guardado la información en {file_name2}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LEER ARCHIVO SIN REPETIDOS Y CON INICIO Y FIN DE RESPUESTAS\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved4.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    datos = json.load(archivo_json)\n",
    "\n",
    "limit_inf = 1\n",
    "limit_sup = 253\n",
    "for data in datos:\n",
    "    if data['count'] >= limit_inf and data['count'] <= limit_sup: \n",
    "        print(f\"\\nCount: {data['count']}\")\n",
    "        #print(f\"Original Report:\\n {data['joined_context']}\\n\")\n",
    "        print(f\"Summary:\\n {data['summary']}\\n\")\n",
    "        texto = data['summary']\n",
    "        print(\"Respuestas:\\n\")\n",
    "        o = 1\n",
    "        for qas in data['qas']:\n",
    "            print(f\"{qas['true_answer']}\\nInicio:{qas['start']} y Fin:{qas['end']}\")       \n",
    "            o+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['count', 'context', 'joined_context', 'summary', 'note_id', 'qas']\n",
      "['true_answer', 'questions', 'start', 'end']\n"
     ]
    }
   ],
   "source": [
    "#ADAPTAR ARCHIVO A BASE DE DATOS DE SQUAD V2.0\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved4.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    base_datos = json.load(archivo_json)\n",
    "\n",
    "print(list(base_datos[0].keys()))\n",
    "print(list(base_datos[0]['qas'][0].keys()))\n",
    "#len(base_datos[0]['qas'][0])\n",
    "#len(base_datos[0]['qas'][0]['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CARGANDO MODELO DE SQUAD V2 PARA OBSERVAR LA ORGANIZACION DE LA DATA\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets keys: ['train', 'validation']\n",
      "Datasets Train keys: ['id', 'title', 'context', 'question', 'answers']\n",
      "Answers keys: dict_keys(['text', 'answer_start'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Datasets keys: {list(dataset.keys())}\")\n",
    "print(f\"Datasets Train keys: {list(dataset['train'][0].keys())}\")\n",
    "print(f\"Answers keys: {dataset['train'][0]['answers'].keys()}\\n\")\n",
    "#print(f\"Dataset Train Value 1:\\n{dataset['train'][:4]}\")\n",
    "#print(f\"Dataset Train Value 1:\\n{dataset['train'][0]}\")\n",
    "#print(f\"Dataset Train Value 1:\\n{dataset['train'][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se escribieron 163695 textos medicos.\n",
      "Se ha guardado la información en data_improved5.json\n"
     ]
    }
   ],
   "source": [
    "#ADAPTAR ARCHIVO A BASE DE DATOS DE SQUAD V2.0 Y ESCRIBIR EN ARCHIVO NUEVO\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved4.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    base_datos = json.load(archivo_json)\n",
    "\n",
    "base_datos_new = []\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "counter = []\n",
    "num_context = 0\n",
    "total_answers = 0\n",
    "for data in base_datos:\n",
    "    num_context += 1\n",
    "    #print(data['count'])\n",
    "    add = {'context_id':num_context,'num_answers':'','num_questions':'',}\n",
    "    num_answers = 0\n",
    "    for qas in data['qas']:\n",
    "        num_answers += 1\n",
    "        total_answers += 1\n",
    "        #print(f\"Inicio: {qas['start']} y fin: {qas['end']}\")\n",
    "        num_questions = 0\n",
    "        for question in qas['questions']:\n",
    "            num_questions += 1\n",
    "            contexts.append(data['summary'])\n",
    "            ans = {'text':[qas['true_answer']],'answer_start':[qas['start']],'answer_end':[qas['end']]}\n",
    "            answers.append(ans)\n",
    "            questions.append(question)\n",
    "            base_datos_new.append({'context':data['summary'],'question':question,'answers': ans})\n",
    "        add['num_questions'] = num_questions\n",
    "    add['num_answers'] = num_answers\n",
    "    counter.append(add)\n",
    "\n",
    "#ESCRIBIENDO NUEVO ARCHIVO CON NUEVA ESTRUCTURA         \n",
    "file_name2 = \"data_improved5.json\"            \n",
    "# Abrir el archivo en modo escritura\n",
    "with open(file_name2, 'w') as archivo_json:\n",
    "    # Escribir el contenido del diccionario en el archivo JSON\n",
    "    json.dump(base_datos_new, archivo_json)\n",
    "\n",
    "print(f\"Se escribieron {len(base_datos_new)} textos medicos.\")\n",
    "print(f\"Se ha guardado la información en {file_name2}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantity of Contexts: 253\n",
      "Quantity of Answers: 4136\n",
      "Quantity of Questions: 163695\n"
     ]
    }
   ],
   "source": [
    "print(f\"Quantity of Contexts: {len(counter)}\")\n",
    "print(f\"Quantity of Answers: {total_answers}\")\n",
    "print(f\"Quantity of Questions: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'question', 'answers'],\n",
      "        num_rows: 130956\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['context', 'question', 'answers'],\n",
      "        num_rows: 32739\n",
      "    })\n",
      "})\n",
      "Tipo Dataset:\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "Tipo Dataset['train']:\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORMACION A OBJETO DATASET Y GUARDADO EN ARCHIVO NUEVO\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved5.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    base_datos = json.load(archivo_json)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, validation_data = train_test_split(base_datos, test_size=0.2, random_state=42)\n",
    "\n",
    "dataset_raw_train = {'context':[],'question':[], 'answers':[]}\n",
    "for data in train_data:\n",
    "    dataset_raw_train['context'].append(data['context'])\n",
    "    dataset_raw_train['question'].append(data['question'])\n",
    "    dataset_raw_train['answers'].append(data['answers'])\n",
    "\n",
    "dataset_raw_validation = {'context':[],'question':[], 'answers':[]}\n",
    "for data in validation_data:\n",
    "    dataset_raw_validation['context'].append(data['context'])\n",
    "    dataset_raw_validation['question'].append(data['question'])\n",
    "    dataset_raw_validation['answers'].append(data['answers'])    \n",
    "    \n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data = {'train': Dataset.from_dict(dataset_raw_train), 'validation': Dataset.from_dict(dataset_raw_validation)}\n",
    "dataset = DatasetDict(data)\n",
    "print(\"Dataset:\")\n",
    "print(dataset)\n",
    "print(\"Tipo Dataset:\")\n",
    "print(type(dataset))\n",
    "print(\"Tipo Dataset['train']:\")\n",
    "print(type(dataset['train']))\n",
    "\n",
    "# Guardar el DatasetDict en un archivo usando pickle\n",
    "with open(\"medical_dataset.pkl\", \"wb\") as archivo:\n",
    "    pickle.dump(dataset, archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [\"The patient was admitted on 5/5/2006 with a history of mechanical fall, with the attending physician being Dr. Clemente Armand Bolstad, with a full code status and disposition of Rehabilitation. Medications on Admission included Amiodarone 100 QD, Colace 100 bid, lasix 40mg QD, Glyburide 5mg bid, Plaquenil 200mg bid, Isordil 20mg tid, Lisinopril 20mg QD, Coumadin 5mg 3dys/week, 2.5mg 4dys/week, Norvasc 10mg QD, Neurontin 300mg TID, with APAP prn. An override was added on 10/2/06 by Gerad E. Dancy, PA for POTENTIALLY SERIOUS INTERACTION: AMIODARONE HCL & WARFARIN with the reason for override being monitoring. The patient was rehydrated with IVF and PO's were encouraged, holding Glypizide while in house, Novolog sliding scale was started on 1/2, Low dose NPH 6 units BID was started on 1/2, bridged with lovenox and INR therapeutic 1/2 and restarted on home regimen of 5/2.5mg variable dose. Pain was controlled with TYLENOL (ACETAMINOPHEN) 650 MG PO Q4H PRN Pain, Headache. A CT pelvis showed a right adnexal cyst which will need further characterization by US and outpatient follow up. The patient has an extensive cardiac history and the fall is not likely related to a cardiac issue as it appears mechanical, with no syncope, chest pain, etc. She was diagnosed with an NSTEMI with a small TnI leak, likely demand related in the setting of hypovolemia and the fall. Enzymes trended down. She was dry on admission and rehydrated with IVF, PO's encouraged, and became euvolemic by 1/2. Her JVP was up to 12cm, although it was difficult to gauge her volume status due to TR. She had a prolonged QT on admission, on telemetry, of unclear etiology, possibly starvation. This was monitored on telemetry until ROMI and drugs that confound were avoided. The QTc resolved to low 500s and a DDD pacer was functioning with V-pacing at 60bpm. Additional medications included NATURAL TEARS (ARTIFICIAL TEARS) 2 DROP OU BID, COLACE (DOCUSATE SODIUM) 100 MG PO BID, PLAQUENIL SULFATE (HYDROXYCHLOROQUINE) 200 MG PO BID, ISORDIL (ISOSORBIDE DINITRATE) 20 MG PO TID, LISINOPRIL 20 MG PO DAILY HOLD IF: SBP <110, MILK OF MAGNESIA (MAGNESIUM HYDROXIDE) 30 MILLILITERS PO DAILY PRN Constipation, COUMADIN (WARFARIN SODIUM) 2.5 MG PO QPM, NORVASC (AMLODIPINE) 10 MG PO DAILY HOLD IF: SBP <110, NEURONTIN (GABAPENTIN) 300 MG PO TID, NEXIUM (ESOMEPRAZOLE) 20 MG PO DAILY, MAALOX-TABLETS QUICK DISSOLVE/CHEWABLE 1-2 TAB PO Q6H PRN Upset Stomach, DULCOLAX RECTAL (BISACODYL RECTAL) 10 MG PR DAILY PRN Constipation, CLOTRIMAZOLE 1% TOPICAL TOPICAL TP BID, GLYBURIDE 5 MG PO BID, LASIX (FUROSEMIDE) 20 MG PO DAILY, and corrected pt restarted on lasix 20 qd on d/c. A PT consult was obtained 3/21 and to follow daily at rehab. Labs showed Na 146, CK 3320, CKMB 12.9, Trop 0.23--->0.10, AST 107, Cr 1.2-->1.6. Pain was controlled with TYLENOL (ACETAMINOPHEN) 650 MG PO Q4H PRN Pain, Headache, rehydrated with IVF, po's encouraged, holding Glypizide while in house, Novolog sliding scale was started on 1/2, Low dose NPH 6 units BID was started on 1/2, bridged with lovenox and INR therapeutic 1/2 and restarted on home regimen of 5/2.5mg variable\", 'This 57-year-old female with a distant history of ovarian cancer, rheumatoid arthritis with systemic lupus erythematosus features, and history of TTP, status post splenectomy, was admitted with fever, shortness of breath, and pleuritic chest pain. She was initially given cefuroxime and levofloxacin in the emergency department for a presumed community acquired pneumonia, as well as Lasix. Her medications included diltiazem 240 mg a day, lisinopril 40 mg a day, Naprosyn 500 mg b.i.d., NPH insulin 24 units subcutaneously q.a.m., Entex-LA, and Cardizem-CD 240 mg p.o. q.d. She underwent thoracentesis and multiple bilateral therapeutic pleuracentesis, and was diuresed aggressively with Lasix, with her oxygen requirement being down from initially 5 to 6 liters per nasal cannula prior to discharge. A continuous Doppler wave form was found and she underwent abdominal CT scan, which did not show any evidence of venous or lymphatic obstruction. Initially, she was started on cefuroxime and azithromycin by the General Medicine team, and her Legionella urine antigen became positive and levofloxacin was added given recommendations from the Infectious Disease Service. She was off of O2 except that she had desaturations to 86% with ambulation, therefore, she was discharged home with p.r.n. oxygen, on Lasix 80 mg b.i.d., insulin sliding scale, lisinopril 40 mg a day, and Cardizem-CD 240 mg p.o. q.d. and levofloxacin 500 mg times 14 days. An elevated platelet count up to 800 and an elevated CA-125 level was discussed with her GYN oncologist, and she was to follow-up with her doctor in one week.'], 'question': ['What is her current dose of lasix ( furosemide )', 'Is the patient currently or have they ever taken entex-la'], 'answers': [{'answer_end': [2598], 'answer_start': [2564], 'text': ['LASIX (FUROSEMIDE) 20 MG PO DAILY,']}, {'answer_end': [541], 'answer_start': [532], 'text': ['Entex-LA,']}]}\n"
     ]
    }
   ],
   "source": [
    "# CARGAR DatasetDict DESDE EL ARCHIVO QUE GUARDO EL OBJETO DATASET\n",
    "with open(\"medical_dataset.pkl\", \"rb\") as archivo:\n",
    "    dataset_cargado = pickle.load(archivo)\n",
    "    \n",
    "print(dataset_cargado['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LEER BASE DE DATOS ADAPTADA A SQUAD V2.0-----INICIO DE FINE TUNNING--------------------------------------------\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Nombre del archivo JSON que deseas leer\n",
    "file_name = \"data_improved5.json\"\n",
    "\n",
    "# Abrir el archivo en modo lectura\n",
    "with open(file_name, 'r') as archivo_json:\n",
    "    # Cargar los datos JSON del archivo en una estructura de datos de Python\n",
    "    base_datos = json.load(archivo_json)\n",
    "    \n",
    "#for data in base_datos[:2]:\n",
    "#    print(f\"Context: {data['context']}\")\n",
    "#    print(f\"Question: {data['question']}\")\n",
    "#    print(f\"Answer: {data['answers']['text']}, Start: {data['answers']['answer_start']}, End: {data['answers']['answer_end']}\\n\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(base_datos, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FIND MAX LENGTHS FOR TOKENIZE\n",
    "import matplotlib.pyplot as plt\n",
    "#Text lengths to contextx\n",
    "token_lens = []\n",
    "train_contexts = []\n",
    "\n",
    "for train in train_data:\n",
    "    train_contexts.append(train[\"context\"])\n",
    "\n",
    "for txt in train_contexts:\n",
    "    txt = txt.strip()  # remove leading and trailing whitespaces\n",
    "    token_lens.append(len(txt.split(' ')))\n",
    "  \n",
    "\n",
    "print(max(token_lens))\n",
    "\n",
    "plt.hist(token_lens,  bins=20)  # density=False would make counts\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Length')\n",
    "plt.title('Distribution of Context Lengths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "130956\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test lengths of Questions\n",
    "token_lens2 = []\n",
    "train_questions = []\n",
    "\n",
    "for train in train_data:\n",
    "    train_questions.append(train[\"question\"])\n",
    "    \n",
    "for txt in train_questions:\n",
    "    txt = txt.strip()  # remove leading and trailing whitespaces\n",
    "    token_lens2.append(len(txt.split(' ')))\n",
    "\n",
    "\n",
    "print(max(token_lens2))\n",
    "print(len(token_lens2))\n",
    "\n",
    "plt.hist(token_lens2,  bins=20)  # density=False would make counts\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Length')\n",
    "plt.title('Distribution of Question Lengths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carga el modelo BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "# Carga el tokenizador\n",
    "model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica si CUDA (GPU) está disponible y envía el modelo a la GPU si es el caso\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Trunca o rellena las secuencias según la longitud máxima\n",
    "    inputs = tokenizer(\n",
    "        data[\"context\"],\n",
    "        data[\"question\"],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=\"only_first\",#\"longest_first\",\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    targets = {\"start_positions\": torch.tensor(data['answers']['answer_start']), \"end_positions\": torch.tensor(data['answers']['answer_end'])}\n",
    "    return inputs, targets\n",
    "\n",
    "train_data = [preprocess_data(entry) for entry in train_data]\n",
    "test_data = [preprocess_data(entry) for entry in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./squad2_fine_tuned_model\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10_000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")\n",
    "\n",
    "# Ejemplo de ajuste del tamaño del lote\n",
    "training_args.per_device_train_batch_size = 4  # Ajusta según sea necesario\n",
    "print(training_args.per_device_train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eladio\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 130956\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65478\n",
      "  Number of trainable parameters = 107721218\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\transformers\\trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1524\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1526\u001b[0m )\n\u001b[1;32m-> 1527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1528\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1529\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1530\u001b[0m     trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1532\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[0;32m   1748\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1750\u001b[0m \n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_trained_in_current_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1753\u001b[0m         steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\transformers\\trainer_utils.py:696\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[0;32m    695\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator(features)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\transformers\\data\\data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\transformers\\data\\data_collator.py:109\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[1;32m--> 109\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mvars\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m    110\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\thesis_env\\Lib\\site-packages\\transformers\\data\\data_collator.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[1;32m--> 109\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mvars\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m    110\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PROBANDO PRETRAINED FINE TUNNING DE HUGGING FACE\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PROBANDO PRETRAINED FINE TUNNING DE HUGGING FACE\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PROBANDO PRETRAINED FINE TUNNING DE HUGGING FACE\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datos[1][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_base_ans_simplify[188]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if len(particiones) == 1:\n",
    "        print(\"Resumen de la unica parte\")\n",
    "        data_paragraph[\"summary\"] = particiones[0]\n",
    "    else:     \n",
    "        summaries = []\n",
    "        j = 0\n",
    "        for i in range(0, len(particiones)): \n",
    "            prompt = ''            \n",
    "            if i == 0:\n",
    "                print(f\"Resumen {i+1}\")\n",
    "                prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "                Medical Report:\\n {particiones[i]+'\\n'+particiones[i+1]} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"            \n",
    "            else:\n",
    "                print(f\"Resumen {i+1}\")\n",
    "                prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "                Medical Report:\\n {summaries[j]+'\\n'+particiones[i+1]} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"\n",
    "                j+=1\n",
    "                         \n",
    "            if i+1 == len(particiones)-1: \n",
    "                response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=1000,temperature=0.6)# Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "                summary = response.choices[0].text.strip() # Extrae la respuesta predicha del resultado de la API\n",
    "                summaries.append(summary)                \n",
    "                break\n",
    "            else:\n",
    "                response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=700,temperature=0.6)# Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "                summary = response.choices[0].text.strip() # Extrae la respuesta predicha del resultado de la API\n",
    "                summaries.append(summary)                \n",
    "        \n",
    "        print(f\"Guardado data_paragraph['summary'] =summaries[{len(summaries)-1}]\")\n",
    "        data_paragraph[\"summary\"] = summaries[len(summaries)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Paso 1: Dividir a la mitad pero en algun valor de '\\n' el array que tiene \"textos\" que superan tokens a resumir. Cada parte debe analizar las respuestas y deben estar incluidas en cada resumen respectivamente a como correspondan. \n",
    "    if data_paragraph[\"count\"] in [0,6,7,8,9,11,12,14,16,17,18,21,22,25,26,27,28,30,31,32,33,34,36,38,39,40,41,42,43,44,46,48,49,50,52,57,58,60,61,62,64, 65, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79, 80, 82, 83, 84, 85, 86, 89, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 123, 124, 125, 128, 129, 130, 132, 133, 136, 137, 138, 139, 141, 142, 143, 146, 147, 148, 149, 150, 151, 152, 155, 156, 157, 158, 160, 161, 164, 165, 167, 168, 170, 172, 174, 176, 177, 178, 179, 180, 181, 184, 185, 186, 187, 189, 190, 192, 196, 199, 200, 203, 204, 206, 207, 208, 210, 212, 218, 219, 222, 223, 225, 226, 227, 229, 230, 232, 233, 234, 236, 237, 238, 240, 241, 245, 247, 248, 249, 250, 251, 252, 254, 257, 258, 259, 260, 261]:#Lista de id de prompts con tokens suficientes [Aprobado]    \n",
    "        #Resumen inicial\n",
    "        prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "        Medical Report:\\n {paragraph[\"joined_context\"]} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"              \n",
    "\n",
    "        # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "        response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=1000,temperature=0.6)\n",
    "        # Extrae la respuesta predicha del resultado de la API\n",
    "        data_paragraph[\"summary\"] = response.choices[0].text.strip()  \n",
    "\n",
    "    elif data_paragraph[\"count\"] in [1,2,3,4,5,10,13,15,19,20,23,24,29,35,37,45,47,51,53,54,55,56,59,63,66, 67, 70, 78, 81, 87, 88, 90, 92, 105, 115, 121, 122, 126, 127, 131, 134, 135, 140, 144, 145, 148, 153, 154, 159, 162, 163, 164, 166, 169, 171, 173, 175, 182, 183, 188, 191, 193, 194, 195, 197, 198, 201, 202, 205, 209, 211, 213, 214, 215, 216, 217, 220, 221, 224, 228, 231, 235, 239, 242, 243, 244, 246, 253, 255, 256]:#Lista de id de prompts con mas tokens de los debidos [Denegado]\n",
    "        # La lista dada\n",
    "        lista = paragraph[\"context\"]\n",
    "\n",
    "        # Dividir la lista a la mitad\n",
    "        mitad = len(lista) // 2\n",
    "        primera_mitad = ''.join(lista[:mitad])\n",
    "        segunda_mitad = ''.join(lista[mitad:])\n",
    "\n",
    "        #PRIMERA MITAD\n",
    "        prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "        Medical Report:\\n {primera_mitad} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"\n",
    "\n",
    "        # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "        response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=700,temperature=0.6)\n",
    "        # Extrae la respuesta predicha del resultado de la API\n",
    "        primera_mitad_resumida = response.choices[0].text.strip()          \n",
    "\n",
    "        #SEGUNDA MITAD\n",
    "        prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "        Medical Report:\\n {segunda_mitad} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"\n",
    "\n",
    "        # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "        response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=700,temperature=0.6)\n",
    "        # Extrae la respuesta predicha del resultado de la API\n",
    "        segunda_mitad_resumida = response.choices[0].text.strip()          \n",
    "\n",
    "        #Paso 1.1: Luego se toman como entrada el resumen de cada parte y las respuestas y debe salir un solo resumen donde se encuentren expresadas las respuestas.\n",
    "        prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "        Medical Report:\\n {primera_mitad_resumida} \\n {segunda_mitad_resumida} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"\n",
    "\n",
    "        # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "        response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=1000,temperature=0.6)\n",
    "        # Extrae la respuesta predicha del resultado de la API\n",
    "        data_paragraph[\"summary\"] = response.choices[0].text.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_aux = data_base_ans_simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_base_ans_simplify[0][\"qas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AHORA VIENEN LOS RESUMENES DE LOS REPORTES MEDICOS\n",
    "#POSTERIORMENTE VIENEN EL CONJUNTO DE PREGUNTAS QUE DEBEN DAR UNA SOLA RESPUESTA DEL RESUMEN CON SPAN EXTRACTION Y SUSTITUIR ESA RESPUESTA EN EL DICCIONARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lista de parrafos, obtenidas de 'data_base', con respuestas NO repetidas para poder realizar el resumen de la manera mas efectiva posible\n",
    "data_clean_ans_duplicates = []\n",
    "for paragraph in data_base:\n",
    "    data_clean = {\n",
    "    'context':'',\n",
    "    'joined_context':'',\n",
    "    'note_id':'',\n",
    "    'answers':[],\n",
    "    }\n",
    "    \n",
    "    data_clean[\"context\"] = paragraph[\"context\"]\n",
    "    data_clean[\"joined_context\"] = paragraph[\"joined_context\"]\n",
    "    data_clean[\"note_id\"] = paragraph[\"note_id\"]\n",
    "    #print(\"ID:\"+paragraph[\"note_id\"]+\"\\n\")\n",
    "    #print(\"CONTEXT:\\n\"+paragraph[\"context\"])\n",
    "    \n",
    "    for qas_item in paragraph[\"qas\"]:\n",
    "        if qas_item[\"true_answer\"] not in data_clean[\"answers\"]:\n",
    "            data_clean[\"answers\"].append(qas_item[\"true_answer\"])\n",
    "        #print(\"*\" * 50)\n",
    "        #print(\"ANSWER TRUE:\"+qas_item[\"true_answer\"])\n",
    "    data_clean_ans_duplicates.append(data_clean)\n",
    "    \n",
    "# Visualizar parrafos con respuestas no repetidas\n",
    "#for paragraph in data_clean_ans_duplicates:\n",
    "#    print(\"ID:\"+paragraph[\"note_id\"]+\"\\n\")\n",
    "#    print(\"CONTEXT:\\n\"+paragraph[\"joined_context\"])  \n",
    "#    for answer in paragraph[\"answers\"]:\n",
    "#        print(\"*\" * 50)\n",
    "#        print(\"ANSWER:\"+answer)    \n",
    "\n",
    "# Longitud de la lista\n",
    "#print(\"Length of 'data_clean_ans_duplicates': \"+str(len(data_clean_ans_duplicates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizar parrafos con respuestas no repetidas\n",
    "for paragraph in data_clean_ans_duplicates[:2]:\n",
    "    print(\"ID:\"+paragraph[\"note_id\"]+\"\\n\")\n",
    "    print(\"CONTEXT:\\n\"+paragraph[\"joined_context\"])  \n",
    "    for answer in paragraph[\"answers\"]:\n",
    "        print(\"*\" * 50)\n",
    "        print(\"ANSWER:\"+answer)    \n",
    "\n",
    "# Longitud de la lista\n",
    "print(\"Length of 'data_clean_ans_duplicates': \"+str(len(data_clean_ans_duplicates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funcion para resumir\n",
    "import openai\n",
    "\n",
    "# Configura tu clave de API\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Define la función para generar respuestas\n",
    "def summarize(prompt):\n",
    "    # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.6,\n",
    "        #model=\"gpt-3.5-turbo\"  # Usar el modelo gpt-3.5-turbo\n",
    "    )\n",
    "\n",
    "    # Extrae la respuesta predicha del resultado de la API\n",
    "    mod_answer = response.choices[0].text.strip()\n",
    "    return mod_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lista de parrafos resumidos\n",
    "data_summary = []\n",
    "count= 0\n",
    "for paragraph in data_clean_ans_duplicates:\n",
    "    data_clean = {\n",
    "    'note_id':'',\n",
    "    'answers':[],\n",
    "    'prompt':'',\n",
    "    'context':'',\n",
    "    'joined_context':'',\n",
    "    'tokens_good':1,\n",
    "    'count':count\n",
    "    }\n",
    "\n",
    "    data_clean[\"note_id\"] = paragraph[\"note_id\"]\n",
    "    data_clean[\"answers\"] = paragraph[\"answers\"]\n",
    "    \n",
    "    answers = ''\n",
    "    o = 1\n",
    "    for answer in paragraph[\"answers\"]:\n",
    "        answers = answers+\"\\n\"+f\"{o}. \"+answer\n",
    "        o+=1\n",
    "    \n",
    "    #print(\"Vamos por: \"+str(count))\n",
    "    prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "    Medical Report:\\n {paragraph[\"joined_context\"]} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"      \n",
    "    # Divide el prompt en tokens separados por espacio\n",
    "    tokens = prompt.split()\n",
    "    # Calcula la cantidad de tokens\n",
    "    token_count = len(tokens)\n",
    "    # Verifica si el token_count excede el límite\n",
    "    if token_count > 4097:\n",
    "        #print(\"El prompt excede el límite de tokens.\\n\")\n",
    "        #print(paragraph[\"context\"])\n",
    "        #print(paragraph[\"joined_context\"])\n",
    "        pass\n",
    "    else:\n",
    "        #print(f\"El prompt tiene {token_count} tokens.\")  \n",
    "        pass\n",
    "    \n",
    "    #Paso 0: Detectar los Reportes con textos excedentes y los que son suficientes, para separar dos listas.\n",
    "    # Resume el parrafo tomando en cuenta las respuestas que no deben modificarse en el texto\n",
    "    if data_clean[\"count\"] not in [1,2,3,4,5,10,13,15,19,20,23,24,29,35,37,45,47,51,53,54,55,56,59,63,66, 67, 70, 78, 81, 87, 88, 90, 92, 105, 115, 121, 122, 126, 127, 131, 134, 135, 140, 144, 145, 148, 153, 154, 159, 162, 163, 164, 166, 169, 171, 173, 175, 182, 183, 188, 191, 193, 194, 195, 197, 198, 201, 202, 205, 209, 211, 213, 214, 215, 216, 217, 220, 221, 224, 228, 231, 235, 239, 242, 243, 244, 246, 253, 255, 256]:#Lista de id de prompts con mas tokens de los debidos [Denegado]\n",
    "        if data_clean[\"count\"] not in [0,6,7,8,9,11,12,14,16,17,18,21,22,25,26,27,28,30,31,32,33,34,36,38,39,40,41,42,43,44,46,48,49,50,52,57,58,60,61,62,64, 65, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79, 80, 82, 83, 84, 85, 86, 89, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 123, 124, 125, 128, 129, 130, 132, 133, 136, 137, 138, 139, 141, 142, 143, 146, 147, 148, 149, 150, 151, 152, 155, 156, 157, 158, 160, 161, 164, 165, 167, 168, 170, 172, 174, 176, 177, 178, 179, 180, 181, 184, 185, 186, 187, 189, 190, 192, 196, 199, 200, 203, 204, 206, 207, 208, 210, 212, 218, 219, 222, 223, 225, 226, 227, 229, 230, 232, 233, 234, 236, 237, 238, 240, 241, 245, 247, 248, 249, 250, 251, 252, 254, 257, 258, 259, 260, 261]:#Lista de id de prompts con tokens suficientes [Aprobado]\n",
    "            print(\"Todavia falta cubrir el valor: \"+str(count))\n",
    "\n",
    "    #REALIZAR LOS PROMPTS!!!!!!!--------------------------------------------------------------------------------------------------\n",
    "    if data_clean[\"count\"] == 1:\n",
    "        #Paso 1: Dividir a la mitad pero en algun valor de '\\n' el array que tiene \"textos\" que superan tokens a resumir. Cada parte debe analizar las respuestas y deben estar incluidas en cada resumen respectivamente a como correspondan. \n",
    "        if data_clean[\"count\"] in [0,6,7,8,9,11,12,14,16,17,18,21,22,25,26,27,28,30,31,32,33,34,36,38,39,40,41,42,43,44,46,48,49,50,52,57,58,60,61,62,64, 65, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79, 80, 82, 83, 84, 85, 86, 89, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 123, 124, 125, 128, 129, 130, 132, 133, 136, 137, 138, 139, 141, 142, 143, 146, 147, 148, 149, 150, 151, 152, 155, 156, 157, 158, 160, 161, 164, 165, 167, 168, 170, 172, 174, 176, 177, 178, 179, 180, 181, 184, 185, 186, 187, 189, 190, 192, 196, 199, 200, 203, 204, 206, 207, 208, 210, 212, 218, 219, 222, 223, 225, 226, 227, 229, 230, 232, 233, 234, 236, 237, 238, 240, 241, 245, 247, 248, 249, 250, 251, 252, 254, 257, 258, 259, 260, 261]:#Lista de id de prompts con tokens suficientes [Aprobado]    \n",
    "            #Resumen inicial\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {paragraph[\"joined_context\"]} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"              \n",
    "            print(prompt)\n",
    "            break\n",
    "            resumen_inicial = summarize(prompt) #joined_context o context\n",
    "\n",
    "            #Optimizacion de span extraction\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {resumen_inicial} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"             \n",
    "            data_clean[\"context\"] = summarize(prompt) #joined_context o context        \n",
    "\n",
    "        elif data_clean[\"count\"] in [1,2,3,4,5,10,13,15,19,20,23,24,29,35,37,45,47,51,53,54,55,56,59,63,66, 67, 70, 78, 81, 87, 88, 90, 92, 105, 115, 121, 122, 126, 127, 131, 134, 135, 140, 144, 145, 148, 153, 154, 159, 162, 163, 164, 166, 169, 171, 173, 175, 182, 183, 188, 191, 193, 194, 195, 197, 198, 201, 202, 205, 209, 211, 213, 214, 215, 216, 217, 220, 221, 224, 228, 231, 235, 239, 242, 243, 244, 246, 253, 255, 256]:#Lista de id de prompts con mas tokens de los debidos [Denegado]\n",
    "            # La lista dada\n",
    "            lista = paragraph[\"context\"]\n",
    "            # Dividir la lista a la mitad\n",
    "            mitad = len(lista) // 2\n",
    "            primera_mitad = ''.join(lista[:mitad])\n",
    "            segunda_mitad = ''.join(lista[mitad:])\n",
    "\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {primera_mitad} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"\n",
    "            primera_mitad_resumida = summarize(prompt) #joined_context o context\n",
    "\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {segunda_mitad} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"\n",
    "            segunda_mitad_resumida = summarize(prompt) #joined_context o context        \n",
    "\n",
    "            #Paso 1.1: Luego se toman como entrada el resumen de cada parte y las respuestas y debe salir un solo resumen donde se encuentren expresadas las respuestas.\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {primera_mitad_resumida} \\n {segunda_mitad_resumida} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"\n",
    "            resumen = summarize(prompt) #joined_context o context \n",
    "\n",
    "            print(prompt)\n",
    "            print(resumen)\n",
    "\n",
    "            break\n",
    "            #Paso 2: Extraer por span extraction las respuestas que quedaron en cada uno de los summary al comparar con las respuestas que debian quedar, asignar las respuestas que deben quedar al final para que tenga coherencia el span extraction.\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {resumen} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"\n",
    "            data_clean[\"context\"] = summarize(prompt) #joined_context o context \n",
    "\n",
    "            #Optimizacion de span extraction\n",
    "            prompt = f\"\"\"You are a medical field expert in \"span extraction\" and excellent writer. Given a medical report and a set of extracted phrases, generate a coherent summary paragraph of the medical report. The summary should seamlessly integrate the provided phrases exactly as they are, including their capitalization, punctuation, and structure. Avoid using synonyms or paraphrasing while incorporating the phrases. The goal is to ensure that the extracted phrases could be \"span extracted\" from the generated summary. The final summary should be presented as a single and long paragraph in English.\\n \n",
    "            Medical Report:\\n {resumen_inicial} \\n Extracted Phrases:\\n {answers}\\n The output just must be the summary.\"\"\"             \n",
    "            data_clean[\"context\"] = summarize(prompt) #joined_context o context         \n",
    "\n",
    "            print(\"\\nResumen combinado:\")\n",
    "            print(data_clean[\"context\"])\n",
    "            \n",
    "    #print(\"*\" * 50)\n",
    "    #print(\"Prompt:\\n\"+prompt)\n",
    "    #print(\"*\" * 50)\n",
    "    #print(\"Summary: \\n\")\n",
    "    #print(data_clean[\"context\"])\n",
    "    #break\n",
    "    #if count == 1:\n",
    "    #    break\n",
    "    count+=1\n",
    "    data_summary.append(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Paso 2.1: colocar todas las preguntas de una respuesta en una sola lista y no en varias listas con la misma respuesta.\n",
    "    #Paso posterior 3: Fine tunning de varias redes y  comparar resultados\n",
    "    #Documentar todo lo hecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Configura tu clave de API\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Define la función para generar respuestas\n",
    "def generate_responses(data):\n",
    "    input_id = data[\"id\"]\n",
    "    context = ''.join(data[\"context\"])\n",
    "    for qas in data[\"qas\"][:1]: #Quitar [:1] para ejecutar completo\n",
    "        #print(\"Question:\")\n",
    "        #print(qas[\"question\"])\n",
    "        question = qas[\"question\"]\n",
    "        #print(\"\\nAnswer true:\")\n",
    "        #print(qas[\"answer_true\"])    \n",
    "        answer_true = qas[\"answer_true\"]\n",
    "        #print(\"Answer predicted:\")\n",
    "        #print(qas[\"answer_predicted\"])         \n",
    "        answer_predicted = qas[\"answer_predicted\"]\n",
    "    \n",
    "        # Forma el prompt combinando el contexto y la pregunta\n",
    "        #Resume en [inglés] la informacion de todo el reporte en \"Context:\" de manera fluida sin perder informacion relevante y que cumpla con la respuesta exacta de la pregunta realizada literalmente dentro del reporte (Todo en ingles)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Summarizes the information of the entire report in \"Context:\" in a fluid way without losing relevant information and that meets the exact answer of the question asked literally within the report.\n",
    "ID:\\n{input_id}\\n\n",
    "Context:\\n{context}\\n\n",
    "Question:{question}\\n\n",
    "Answer true:{answer_true}\\n\n",
    "Answer predicted:{answer_predicted}\n",
    "\n",
    "The output should have:\\n\n",
    "\n",
    "1) Context: Summary report in paragraph labeled \"Context\"\\n\n",
    "2) Question: Question made with the tag \"Question\"\\n\n",
    "3) Answer true: Span extraction response tagged \"Answer true\". This response must be exact in the \"Context\" for it to fulfill the Span extraction task in the subsequent training of the model.\\n\n",
    "\n",
    "Example: if the answer says: \"ECASA ( ASPIRIN ENTERIC COATED ) 81 MG PO 3x/Week M-W-F\". So the text \"ECASA ( ASPIRIN ENTERIC COATED ) 81 MG PO 3x/Week M-W-F\" must literally be in the paragraph for the span extraction to be fulfilled, therefore nothing in that phrase should be changed, you cannot write \"three times a week\" has to be placed \"3x/Week\" like all the content of the answer tag.\n",
    "\n",
    "4) Answer predicted: which in this case would be empty with brackets [] and would have the label \"Answer predicted\"\\n\n",
    "\n",
    "\"\"\"\n",
    "        #print(prompt)\n",
    "        \n",
    "        # Genera la respuesta predicha utilizando GPT-3.5 Turbo\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=400,\n",
    "            temperature=0.7,\n",
    "            #model=\"gpt-3.5-turbo\"  # Usar el modelo gpt-3.5-turbo\n",
    "        )\n",
    "\n",
    "        # Extrae la respuesta predicha del resultado de la API\n",
    "        mod_answer = response.choices[0].text.strip()\n",
    "        #print(mod_answer)\n",
    "    return mod_answer\n",
    "\n",
    "ans_array = []\n",
    "\n",
    "# Genera las respuestas para cada diccionario en el array\n",
    "for data in data_dictionary2[:2]:\n",
    "    #print(generate_responses(data))\n",
    "    ans_array.append(generate_responses(data))\n",
    "\n",
    "# Imprime los resultados\n",
    "\n",
    "c = 0\n",
    "for item in ans_array:\n",
    "    print(f\"Answer mod {c}:\")\n",
    "    print(item)\n",
    "    print(\"=\" * 50)\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACT INFORMATION OF FILE on data_dictionary\n",
    "data_dictionary = []\n",
    "\n",
    "\n",
    "context_temporal = []\n",
    "context_temporal_join = []\n",
    "\n",
    "for data in dictionary[\"data\"]:\n",
    "    for paragraphs in data[\"paragraphs\"]:\n",
    "        if len(paragraphs['context'][0].split(\"#\")) == 2:\n",
    "            #print(paragraphs['context'][0].split(\"#\")[1].split(\"\\n\")[0])\n",
    "\n",
    "            context_temporal.append(paragraphs['context'])\n",
    "            context_temporal_join.append(''.join(paragraphs['context']))\n",
    "\n",
    "            for qas in paragraphs[\"qas\"]:\n",
    "                if qas[\"answers\"]:\n",
    "                    if qas[\"answers\"][0]['evidence']:\n",
    "                        for question in qas[\"question\"]:\n",
    "                            data_dictionary.append({\"id\": paragraphs['context'][0].split(\"#\")[1].split(\"\\n\")[0], \"context\":paragraphs['context'], \"question\":question, \"answer_true\":qas[\"answers\"][0]['evidence'], \"answer_predicted\":[]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "    ID\n",
    "    Context\n",
    "    Question\n",
    "    Answer true\n",
    "    Amswer predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACT INFORMATION OF FILE on data_dictionary2\n",
    "data_dictionary2 = []\n",
    "qas_aux = []\n",
    "\n",
    "context_temporal2 = []\n",
    "context_temporal_join2 = []\n",
    "\n",
    "for data in dictionary[\"data\"]:\n",
    "    for paragraphs in data[\"paragraphs\"]:\n",
    "        if len(paragraphs['context'][0].split(\"#\")) == 2:\n",
    "            \n",
    "            #print(\"\\n PARADA 1 \\n\")\n",
    "            #print(paragraphs['context'][0])\n",
    "\n",
    "            context_temporal2.append(paragraphs['context'])\n",
    "            #print(\"\\n PARADA 2 \\n\")\n",
    "            #print(paragraphs['context'])\n",
    "            \n",
    "            context_temporal_join2.append(''.join(paragraphs['context']))\n",
    "            #print(\"\\n PARADA 3 \\n\")\n",
    "            #print(''.join(paragraphs['context']))\n",
    "            \n",
    "            info = {\n",
    "                \"id\": paragraphs['context'][0].split(\"#\")[1].split(\"\\n\")[0],\n",
    "                \"context\":paragraphs['context'],\n",
    "                \"qas\":[],\n",
    "            }\n",
    "            \n",
    "            #print(\"\\n PARADA 4 \\n\")\n",
    "            #print(info[\"id\"])\n",
    "            #print(\"\\n PARADA 5 \\n\")\n",
    "            #print(info[\"context\"])\n",
    "            #print(\"\\n PARADA 6 \\n\")\n",
    "            #print(info[\"qas\"])\n",
    "            \n",
    "            for qas in paragraphs[\"qas\"]:\n",
    "                if qas[\"answers\"]:\n",
    "                    if qas[\"answers\"][0]['evidence']:\n",
    "                        for question in qas[\"question\"]:\n",
    "                            qas_aux.append({\"question\":question, \"answer_true\":qas[\"answers\"][0]['evidence'], \"answer_predicted\":[]})\n",
    "            info[\"qas\"] = qas_aux\n",
    "            \n",
    "            #print(\"\\n PARADA 7 \\n\")\n",
    "            #print(info[\"id\"])\n",
    "            #print(\"\\n PARADA 8 \\n\")\n",
    "            #print(info[\"context\"])\n",
    "            #print(\"\\n PARADA 9 \\n\")\n",
    "            #print(info[\"qas\"])\n",
    "            #print(\"\\n PARADA 10 \\n\")\n",
    "            #print(\"question:\")\n",
    "            #print(info[\"qas\"][0][\"question\"])\n",
    "            #print(\"answer_true:\")\n",
    "            #print(info[\"qas\"][0][\"answer_true\"])\n",
    "            #print(\"answer_predicted:\")\n",
    "            #print(info[\"qas\"][0][\"answer_predicted\"])\n",
    "\n",
    "            #break\n",
    "            \n",
    "            data_dictionary2.append(info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "    ID\n",
    "    Context\n",
    "        Question\n",
    "        Answer true\n",
    "        Amswer predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_dictionary2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizar la data de data_dictionary2\n",
    "for data in data_dictionary2[1:2]:#Quitar [:1] para ejecutar completo\n",
    "    print(\"ID:\")\n",
    "    print(data[\"id\"])\n",
    "    print(\"Context:\")\n",
    "    print(''.join(data[\"context\"]))\n",
    "    for qas in data[\"qas\"][1:6]: #Quitar [:1] para ejecutar completo\n",
    "        print(\"Question:\")\n",
    "        print(qas[\"question\"])\n",
    "        print(\"\\nAnswer true:\")\n",
    "        print(qas[\"answer_true\"])\n",
    "        print(\"Answer predicted:\")\n",
    "        print(qas[\"answer_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in context_temporal[:1]:\n",
    "    #print(data[:14])\n",
    "    joined_text = \"It is \"+data[0].split(\"\\n\")[0]+\". \"+\"The admission date is \"\n",
    "    sentence = data[1].split(' ')\n",
    "    date = sentence[len(sentence)-3] \n",
    "    joined_text = joined_text+date\n",
    "    date = data[2].split(':')[1].split(\"\\n\")[0]\n",
    "    joined_text = joined_text+\". The discharge date is \"+date+\".\"\n",
    "    for piece in data:\n",
    "        if piece.split(\":\")[0].lower()==\"attending\":\n",
    "            doctor = piece.split(\":\")[1].split(\"\\n\")[0]\n",
    "    joined_text = joined_text+\" The doctor attending is \"+doctor\n",
    "    c = 0\n",
    "    for piece in data:\n",
    "        if piece.split(\":\")[0].lower()==\"discharge medications\":\n",
    "            #print(piece.split(\":\")[0].lower())\n",
    "            if piece.split(\":\")[1] != \"\\n\": \n",
    "                medications = piece.split(\":\")[1] \n",
    "            elif data[c+1]:\n",
    "                medications = data[c+1].split(\"\\n\")[0]+\".\"\n",
    "        c=c+1\n",
    "    joined_text = joined_text+\" The discharge medications are \"+medications\n",
    "    \n",
    "    c = 0\n",
    "    for piece in data:\n",
    "        #print(piece.split(\":\")[0].lower())\n",
    "        if c>2:\n",
    "            if len(piece.split(\":\"))>1:\n",
    "                joined_text = joined_text+\" The \"+piece.split(\":\")[0]+\" is \"\n",
    "                if piece.split(\":\")[1] != \"\\n\": \n",
    "                    contenido = piece.split(\":\")[1].split(\"\\n\")[0]+\". \"\n",
    "                elif data[c+1]:\n",
    "                    contenido = data[c+1].split(\"\\n\")[0]+\". \"\n",
    "            \n",
    "\n",
    "                joined_text = joined_text+contenido\n",
    "        c=c+1    \n",
    "    #print(sentence)\n",
    "    print(joined_text)\n",
    "    #Revisar cuando es Hora\n",
    "    #Revisar la palabra anterior a colocar \"is\", por si es (in,at,on)\n",
    "    #NER de la palabra sucesiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for data in data_dictionary[:1]:\n",
    "    # print(data[\"context\"])\n",
    "    print(\"Joined context:\")\n",
    "    text = ''.join(data[\"context\"])\n",
    "    print(text)\n",
    "    print(\"Question:\")\n",
    "    print(data[\"question\"])\n",
    "    print(\"\\nAnswer true:\")\n",
    "    print(data[\"answer_true\"])\n",
    "    # print(''.join(data[\"context\"]))\n",
    "    #Convertir a parrafo por summarize o manual para probar las respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THESIS",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

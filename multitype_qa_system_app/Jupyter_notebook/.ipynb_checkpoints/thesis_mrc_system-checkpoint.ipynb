{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research and Implementation of multi-type question answering system in medical field based on machine reading comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import *\n",
    "from transformers import pipeline\n",
    "import scispacy\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPLOAD FILE\n",
    "import json\n",
    "with open(\"data.json\") as archivo:\n",
    "    dictionary = json.load(archivo)\n",
    "#i=1\n",
    "#len(dictionary[\"data\"][i][\"paragraphs\"][i][\"qas\"][i][\"question\"])#[0][\"paraphrase question\"]\n",
    "# dictionary[\"data\"][i][\"paragraphs\"][i][\"note_id\"]\n",
    "# dictionary[\"data\"][i][\"paragraphs\"][i][\"context\"]\n",
    "# dictionary[\"data\"][i][\"paragraphs\"][i][\"qas\"][i][\"question\"]\n",
    "# dictionary[\"data\"][i][\"paragraphs\"][i][\"qas\"][i][\"answers\"]\n",
    "# dictionary[\"data\"][i][\"paragraphs\"][i][\"qas\"][i][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACT INFORMATION OF FILE\n",
    "data_dictionary = []\n",
    "\n",
    "for data in dictionary[\"data\"]:\n",
    "    for paragraphs in data[\"paragraphs\"]:\n",
    "        print(\"Context:\")\n",
    "        print(paragraphs['context'])\n",
    "        for qas in paragraphs[\"qas\"]:\n",
    "            if qas[\"answers\"][0]['evidence']:\n",
    "                print(\"Questions:\")\n",
    "                print(qas[\"question\"])\n",
    "                print(\"Answers:\")\n",
    "                print(\"Evidence: \",qas[\"answers\"][0]['evidence'])\n",
    "                data_dictionary.append({\"context\":paragraphs['context'], \"question\":qas[\"question\"], \"answer_true\":qas[\"answers\"][0]['evidence'], \"answer_predicted\":[]})\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_dictionary))\n",
    "#print(\"Length of the arrays in the dictionary: {len_dictionary}, Length of Contexts: {len_context}, Length of Questions: {len_question} and Length of True answers: {len_answers_true}\".format(len_dictionary = len(data_input[\"context\"]), len_context = len(context), len_question = len(questions), len_answers_true = len(answers_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL IMPLEMENTATION 'deepset/bert-base-cased-squad2'\n",
    "\n",
    "modelname = 'deepset/bert-base-cased-squad2'\n",
    "tokenizer = BertTokenizer.from_pretrained(modelname)\n",
    "model = BertForQuestionAnswering.from_pretrained(modelname)\n",
    "qa = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**MODEL IMPLEMENTATION \"deepset/roberta-base-squad2\"\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "# Initialization\n",
    "batch_size = 96\n",
    "n_epochs = 2\n",
    "base_LM_model = \"roberta-base\"\n",
    "max_seq_len = 386\n",
    "learning_rate = 3e-5\n",
    "# lr_schedule = LinearWarmup\n",
    "warmup_proportion = 0.2\n",
    "doc_stride=128\n",
    "max_query_length=64\n",
    "\n",
    "model_name4 = \"deepset/roberta-base-squad2\"\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(model_name4) # Load tokenizer\n",
    "model4 = AutoModelForQuestionAnswering.from_pretrained(model_name4) # Load model\n",
    "qa4 = pipeline('question-answering', model=model_name4, tokenizer=tokenizer4) # Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**MODEL IMPLEMENTATION \"deepset/roberta-base-squad2-covid\"\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "# Initialization\n",
    "batch_size = 24\n",
    "n_epochs = 3\n",
    "base_LM_model = \"deepset/roberta-base-squad2\"\n",
    "max_seq_len = 384\n",
    "learning_rate = 3e-5\n",
    "# lr_schedule = LinearWarmup\n",
    "warmup_proportion = 0.1\n",
    "doc_stride = 128\n",
    "xval_folds = 5\n",
    "dev_split = 0\n",
    "no_ans_boost = -100\n",
    "\n",
    "model_name5 = \"deepset/roberta-base-squad2-covid\"\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(model_name5) # Load tokenizer\n",
    "model5 = AutoModelForQuestionAnswering.from_pretrained(model_name5) # Load model\n",
    "qa5 = pipeline('question-answering', model=model_name5, tokenizer=tokenizer5) # Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST MODEL\n",
    "for data in data_dictionary[:2]:\n",
    "    data[\"answer_predicted\"] = qa({'question': data[\"question\"][0]+'?', 'context': ''.join(data[\"context\"])})\n",
    "    \n",
    "    print(\"Joined context:\")\n",
    "    print(''.join(data[\"context\"]))\n",
    "    print(\"Question: \")\n",
    "    print(data[\"question\"][0]+'?')    \n",
    "    print(\"Answer predicted: \")\n",
    "    print(data[\"answer_predicted\"])\n",
    "    print(\"Answer True: \")\n",
    "    print(data[\"answer_true\"])    \n",
    "\n",
    "# data_dictionary_test = data_dictionary[:2] #Set the array of data to a new array to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICAS\n",
    "import re\n",
    "data_dictionary_test = [{\"answer_predicted\": {'answer':\"Hola mundo\"}, \"answer_true\": \"Hola mundo\"},\n",
    "          {\"answer_predicted\": {'answer':\"Pepe corrio ayer\"}, \"answer_true\": \"Pepe, corrio ayer.\"},\n",
    "          {\"answer_predicted\": {'answer':\"Marcos el n√∫mero 1!\"}, \"answer_true\": \"marcos el numero 1\"},\n",
    "          {\"answer_predicted\": {'answer':\"Pepe es gordo y feo\"}, \"answer_true\": \"pepe es gordo\"}]\n",
    "\n",
    "#Exact Match\n",
    "em = []\n",
    "for answer in data_dictionary_test: \n",
    "    pred = re.sub('[^0-9a-z ]', '', answer['answer_predicted']['answer'].lower())\n",
    "    true = re.sub('[^0-9a-z ]', '', answer['answer_true'].lower())\n",
    "    if pred == true:\n",
    "        em.append(1)\n",
    "    else:\n",
    "        em.append(0)\n",
    "\n",
    "average = sum(em)/len(em)\n",
    "print(\"Exact match:\")\n",
    "print(average)\n",
    "print('\\n')\n",
    "\n",
    "#ROUGE\n",
    "from rouge import Rouge\n",
    "model_out = [ans['answer_predicted']['answer'] for ans in data_dictionary_test]\n",
    "reference = [ans['answer_true'] for ans in data_dictionary_test]\n",
    "clean = re.compile('(?i)[^0-9a-z ]')\n",
    "model_out = [clean.sub('', text.lower()) for text in model_out]\n",
    "reference = [clean.sub('', text.lower()) for text in reference]\n",
    "\n",
    "rouge = Rouge()\n",
    "result_rouge = rouge.get_scores(model_out, reference)\n",
    "result_rouge_avg = rouge.get_scores(model_out, reference, avg=true)\n",
    "scores = rouge.get_scores(model_out, reference)\n",
    "print(\"ROUGE:\")\n",
    "print(result_rouge)\n",
    "print(\"ROUGE AVERAGE:\")\n",
    "print(result_rouge_avg)\n",
    "print(\"rouge[1]:\")\n",
    "print(model_out[1], '|', reference[1], '|', scores[1]['rouge-1']['f'])\n",
    "print('\\n')\n",
    "\n",
    "#F1-Score\n",
    "#RECALL = count(match reference y model gram n)/count(reference gram n)\n",
    "#recall = [print(x) for x in [1,2,3,4,5,6] if x < 5]\n",
    "list_ref = []\n",
    "print(\"F1-SCORE:\")\n",
    "\n",
    "print(\"Reference:\")\n",
    "for ref in reference:\n",
    "    split_ref = ref.split()\n",
    "    list_ref.append(split_ref)\n",
    "\n",
    "print(list_ref)\n",
    "\n",
    "list_model = []\n",
    "print(\"Model:\")\n",
    "for model in model_out:\n",
    "    split_model = model.split()\n",
    "    list_model.append(split_model)\n",
    "    \n",
    "print(list_model)\n",
    "#print(list_model[0])\n",
    "i = 0\n",
    "list_match = []\n",
    "for phrase in list_ref:\n",
    "    count = 0\n",
    "    for word in phrase:\n",
    "        if word in list_model[i]:\n",
    "            count = count + 1\n",
    "    list_match.append(count)\n",
    "    i = i+1\n",
    "\n",
    "list_countref = []\n",
    "list_countmodel = []\n",
    "#count ngram\n",
    "for phrase in list_ref:\n",
    "    list_countref.append(len(phrase))\n",
    "for phrase in list_model:\n",
    "    list_countmodel.append(len(phrase))\n",
    "print(\"Count words ref:\")\n",
    "print(list_countref)\n",
    "print(\"Count words model:\")\n",
    "print(list_countmodel)\n",
    "print(\"Count words match:\")\n",
    "print(list_match)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "match = np.array(list_match)\n",
    "count_ref = np.array(list_countref)\n",
    "count_model = np.array(list_countmodel)\n",
    "\n",
    "array_recall = match/count_ref\n",
    "array_precision = match/count_model\n",
    "print(\"Recall array:\")\n",
    "print(array_recall)\n",
    "print(\"Precision array:\")\n",
    "print(array_precision)\n",
    "\n",
    "num = 2*array_precision*array_recall\n",
    "den = array_precision + array_recall\n",
    "\n",
    "f1score = num/den\n",
    "print(\"F1-Score:\")\n",
    "print(f1score)\n",
    "\n",
    "#ROUGE-L --> Longest Common Secuence\n",
    "#ROUGE-S --> Skip gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBANDO MODELO PARA UNA SOLA PREGUNTA CON NER Y VISUALIZAR RENDER CON SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Peter Bush has a house in Mars, it is close to ONU, Church Chicken and OEA. Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity. They accumulate in tumor-bearing mice and humans \n",
    "with different types of cancer, including hepatocellular carcinoma (HCC). Peter was prescribed Magnesium hydroxide 400mg/5ml suspension PO of total 30ml bid for the next 5 days.\n",
    "\"\"\"\n",
    "question = \"What was Peter prescribed?\"\n",
    "ans = qa({'question': question, 'context': context})\n",
    "ans['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med7 = spacy.load(\"en_core_med7_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create distinct colours for labels\n",
    "col_dict = {}\n",
    "seven_colours = ['#e6194B', '#3cb44b', '#ffe119', '#ffd8b1', '#f58231', '#f032e6', '#42d4f4']\n",
    "for label, colour in zip(med7.pipe_labels['ner'], seven_colours):\n",
    "    col_dict[label] = colour\n",
    "\n",
    "options = {'ents': med7.pipe_labels['ner'], 'colors':col_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp1(context)\n",
    "doc1.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp2(context)\n",
    "doc2.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = med7(context)\n",
    "doc3.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc1.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc2.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc3.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.ents[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clasificacion de la respuesta\n",
    "#Habran varias entidades para un mismo texto, pero no habra como seleccionar entre ellas porque no tendran score de entidades, y la respuesta que si tiene score es unico.\n",
    "#Ejemplo: Magnesium hydroxide CHEMICAL, Magnesium hydroxide DRUG. No hay como definir si tomas uno o el otro...\n",
    "docs = [doc1, doc2, doc3]\n",
    "answers=[]\n",
    "for d in docs:\n",
    "    #score = 0\n",
    "    for entity in d.ents:\n",
    "        if entity.text == ans['answer']:\n",
    "            answers.append(entity)\n",
    "            # print(entity.label_)\n",
    "            \n",
    "answers_filter = list(set(answers))\n",
    "if len(answers_filter) > 1:\n",
    "    answer_unique = answers_filter[0]\n",
    "elif len(answers_filter) == 0:\n",
    "    answer_unique = 'None'\n",
    "elif len(answers_filter) == 1:\n",
    "    answer_unique = answers_filter\n",
    "\n",
    "text = answer_unique.text\n",
    "label = answer_unique.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question)\n",
    "print(f\"Answer: ({text}, {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc1, style='ent')\n",
    "displacy.render(doc2, style='ent')\n",
    "spacy.displacy.render(doc3, style='ent', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LATER TRY TO DO IT\n",
    "#DistilBert Model\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',return_token_type_ids = True)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "context = \"The US has passed the peak on new coronavirus cases, \" \\\n",
    "          \"President Donald Trump said and predicted that some states would reopen this month. \" \\\n",
    "          \"The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\"\n",
    "\n",
    "question = \"What was President Donald Trump's prediction?\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(question, context)\n",
    "\n",
    "\n",
    "input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "start_scores, end_scores = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "\n",
    "ans_tokens = input_ids[torch.argmax(start_scores) : torch.argmax(end_scores)+1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(ans_tokens , skip_special_tokens=True)\n",
    "\n",
    "print (\"\\nQuestion \",question)\n",
    "print (\"\\nAnswer Tokens: \")\n",
    "print (answer_tokens)\n",
    "\n",
    "answer_tokens_to_string = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "print (\"\\nAnswer : \",answer_tokens_to_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
